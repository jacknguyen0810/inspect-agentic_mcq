
@misc{noauthor_gemini_2025,
	title = {Gemini 2.5: {Our} most intelligent {AI} model},
	shorttitle = {Gemini 2.5},
	url = {https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/},
	abstract = {Gemini 2.5 is our most intelligent AI model, now with thinking.},
	language = {en-us},
	urldate = {2025-05-29},
	journal = {Google},
	month = mar,
	year = {2025},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\DYSD6DF6\\gemini-model-thinking-updates-march-2025.html:text/html},
}

@misc{skarlinski_language_2024,
	title = {Language agents achieve superhuman synthesis of scientific knowledge},
	url = {http://arxiv.org/abs/2409.13740},
	doi = {10.48550/arXiv.2409.13740},
	abstract = {Language models are known to ‚Äúhallucinate‚Äù incorrect information, and it is unclear if they are sufficiently accurate and reliable for use in scientific research. We developed a rigorous human-AI comparison methodology to evaluate language model agents on real-world literature search tasks covering information retrieval, summarization, and contradiction detection tasks. We show that PaperQA2, a frontier language model agent optimized for improved factuality, matches or exceeds subject matter expert performance on three realistic literature research tasks without any restrictions on humans (i.e., full access to internet, search tools, and time). PaperQA2 writes cited, Wikipediastyle summaries of scientific topics that are significantly more accurate than existing, human-written Wikipedia articles. We also introduce a hard benchmark for scientific literature research called LitQA2 that guided design of PaperQA2, leading to it exceeding human performance. Finally, we apply PaperQA2 to identify contradictions within the scientific literature, an important scientific task that is challenging for humans. PaperQA2 identifies 2.34 ¬± 1.99 (mean ¬± SD, N = 93 papers) contradictions per paper in a random subset of biology papers, of which 70\% are validated by human experts. These results demonstrate that language model agents are now capable of exceeding domain experts across meaningful tasks on scientific literature.},
	language = {en},
	urldate = {2025-05-29},
	publisher = {arXiv},
	author = {Skarlinski, Michael D. and Cox, Sam and Laurent, Jon M. and Braza, James D. and Hinks, Michaela and Hammerling, Michael J. and Ponnapati, Manvitha and Rodriques, Samuel G. and White, Andrew D.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.13740 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Physics - Physics and Society},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\Z3M4TMG5\\Skarlinski et al. - 2024 - Language agents achieve superhuman synthesis of scientific knowledge.pdf:application/pdf},
}

@misc{gupta_comprehensive_2024,
	title = {A {Comprehensive} {Survey} of {Retrieval}-{Augmented} {Generation} ({RAG}): {Evolution}, {Current} {Landscape} and {Future} {Directions}},
	shorttitle = {A {Comprehensive} {Survey} of {Retrieval}-{Augmented} {Generation} ({RAG})},
	url = {http://arxiv.org/abs/2410.12837},
	doi = {10.48550/arXiv.2410.12837},
	abstract = {This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Gupta, Shailja and Ranjan, Rajesh and Singh, Surya Narayan},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: 4 Figures},
	file = {Preprint PDF:C\:\\Users\\jackn\\Zotero\\storage\\YE6YKMWE\\Gupta et al. - 2024 - A Comprehensive Survey of Retrieval-Augmented Generation (RAG) Evolution, Current Landscape and Fut.pdf:application/pdf;Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\XC6Q8Q4J\\2410.html:text/html},
}

@misc{openai_competitive_2025,
	title = {Competitive {Programming} with {Large} {Reasoning} {Models}},
	url = {http://arxiv.org/abs/2502.06807},
	doi = {10.48550/arXiv.2502.06807},
	abstract = {We show that reinforcement learning applied to large language models (LLMs) significantly boosts performance on complex coding and reasoning tasks. Additionally, we compare two general-purpose reasoning models ‚Äî OpenAI o1 and an early checkpoint of o3 ‚Äî with a domain-specific system, o1ioi, which uses hand-engineered inference strategies designed for competing in the 2024 International Olympiad in Informatics (IOI). We competed live at IOI 2024 with o1-ioi and, using hand-crafted test-time strategies, placed in the 49th percentile. Under relaxed competition constraints, o1-ioi achieved a gold medal. However, when evaluating later models such as o3, we find that o3 achieves gold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that although specialized pipelines such as o1-ioi yield solid improvements, the scaled-up, general-purpose o3 model surpasses those results without relying on hand-crafted inference heuristics. Notably, o3 achieves a gold medal at the 2024 IOI and obtains a CodeForces rating on par with elite human competitors. Overall, these results indicate that scaling general-purpose reinforcement learning, rather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in reasoning domains, such as competitive programming.},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {OpenAI and El-Kishky, Ahmed and Wei, Alexander and Saraiva, Andre and Minaiev, Borys and Selsam, Daniel and Dohan, David and Song, Francis and Lightman, Hunter and Clavera, Ignasi and Pachocki, Jakub and Tworek, Jerry and Kuhn, Lorenz and Kaiser, Lukasz and Chen, Mark and Schwarzer, Max and Rohaninejad, Mostafa and McAleese, Nat and contributors, o3 and M√ºrk, Oleg and Garg, Rhythm and Shu, Rui and Sidor, Szymon and Kosaraju, Vineet and Zhou, Wenda},
	month = feb,
	year = {2025},
	note = {arXiv:2502.06807 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\6XUDLABG\\OpenAI et al. - 2025 - Competitive Programming with Large Reasoning Models.pdf:application/pdf},
}

@misc{shojaee_llm-srbench_2025,
	title = {{LLM}-{SRBench}: {A} {New} {Benchmark} for {Scientific} {Equation} {Discovery} with {Large} {Language} {Models}},
	shorttitle = {{LLM}-{SRBench}},
	url = {http://arxiv.org/abs/2504.10415},
	doi = {10.48550/arXiv.2504.10415},
	abstract = {Scientific equation discovery is a fundamental task in the history of scientific progress, enabling the derivation of laws governing natural phenomena. Recently, Large Language Models (LLMs) have gained interest for this task due to their potential to leverage embedded scientific knowledge for hypothesis generation. However, evaluating the true discovery capabilities of these methods remains challenging, as existing benchmarks often rely on common equations that are susceptible to memorization by LLMs, leading to inflated performance metrics that do not reflect discovery. In this paper, we introduce LLM-SRBench, a comprehensive benchmark with 239 challenging problems across four scientific domains specifically designed to evaluate LLM-based scientific equation discovery methods while preventing trivial memorization. Our benchmark comprises two main categories: LSR-Transform, which transforms common physical models into less common mathematical representations to test reasoning beyond memorized forms, and LSR-Synth, which introduces synthetic, discovery-driven problems requiring data-driven reasoning. Through extensive evaluation of several state-of-the-art methods, using both open and closed LLMs, we find that the best-performing system so far achieves only 31.5\% symbolic accuracy. These findings highlight the challenges of scientific equation discovery, positioning LLM-SRBench as a valuable resource for future research.},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Shojaee, Parshin and Nguyen, Ngoc-Hieu and Meidani, Kazem and Farimani, Amir Barati and Doan, Khoa D. and Reddy, Chandan K.},
	month = apr,
	year = {2025},
	note = {arXiv:2504.10415 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Project page: https://github.com/deep-symbolic-mathematics/llm-srbench , Benchmark page: https://huggingface.co/datasets/nnheui/llm-srbench},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\GGYM6AIM\\Shojaee et al. - 2025 - LLM-SRBench A New Benchmark for Scientific Equation Discovery with Large Language Models.pdf:application/pdf},
}

@misc{cai_sciassess_2024,
	title = {{SciAssess}: {Benchmarking} {LLM} {Proficiency} in {Scientific} {Literature} {Analysis}},
	shorttitle = {{SciAssess}},
	url = {http://arxiv.org/abs/2403.01976},
	doi = {10.48550/arXiv.2403.01976},
	abstract = {Recent breakthroughs in Large Language Models (LLMs) have revolutionized scientific literature analysis. However, existing benchmarks fail to adequately evaluate the proficiency of LLMs in this domain, particularly in scenarios requiring higher-level abilities beyond mere memorization and the handling of multimodal data. In response to this gap, we introduce SciAssess, a benchmark specifically designed for the comprehensive evaluation of LLMs in scientific literature analysis. It aims to thoroughly assess the efficacy of LLMs by evaluating their capabilities in Memorization (L1), Comprehension (L2), and Analysis \& Reasoning (L3). It encompasses a variety of tasks drawn from diverse scientific fields, including biology, chemistry, material, and medicine. To ensure the reliability of SciAssess, rigorous quality control measures have been implemented, ensuring accuracy, anonymization, and compliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting their strengths and areas for improvement. We hope this evaluation supports the ongoing development of LLM applications in scientific literature analysis. SciAssess and its resources are available at https: //github.com/sci-assess/SciAssess.},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Cai, Hengxing and Cai, Xiaochen and Chang, Junhan and Li, Sihang and Yao, Lin and Wang, Changxin and Gao, Zhifeng and Wang, Hongshuai and Li, Yongge and Lin, Mujie and Yang, Shuwen and Wang, Jiankun and Xu, Mingjun and Huang, Jin and Fang, Xi and Zhuang, Jiaxi and Yin, Yuqi and Li, Yaqi and Chen, Changhong and Cheng, Zheng and Zhao, Zifeng and Zhang, Linfeng and Ke, Guolin},
	month = oct,
	year = {2024},
	note = {arXiv:2403.01976 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\VAIZ2GIF\\Cai et al. - 2024 - SciAssess Benchmarking LLM Proficiency in Scientific Literature Analysis.pdf:application/pdf},
}

@misc{quan_codeelo_2025,
	title = {{CodeElo}: {Benchmarking} {Competition}-level {Code} {Generation} of {LLMs} with {Human}-comparable {Elo} {Ratings}},
	shorttitle = {{CodeElo}},
	url = {http://arxiv.org/abs/2501.01257},
	doi = {10.48550/arXiv.2501.01257},
	abstract = {With the increasing code reasoning capabilities of existing large language models (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3, there is a growing need to develop more challenging and comprehensive benchmarks that effectively test their sophisticated competition-level coding abilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to the unavailability of private test cases, lack of support for special judges, and misaligned execution environments. To bridge this gap, we introduce CODEELO, a standardized competition-level code generation benchmark that effectively addresses all these challenges for the first time. CODEELO benchmark is mainly based on the official CodeForces1 platform and tries to align with the platform as much as possible. We compile the recent six months of contest problems on CodeForces with detailed information such as contest divisions, problem difficulty ratings, and problem algorithm tags. We introduce a unique judging method in which problems are submitted directly to the platform and develop a reliable Elo rating calculation system that aligns with the platform and is comparable with human participants but has lower variance. By testing on our CODEELO, we provide the Elo ratings of 30 existing popular open-source and 3 proprietary LLMs for the first time. The results show that o1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of 1578 and 1261, respectively, while other models struggle even with the easiest problems, placing in the lowest 25 percent among all human participants. Detailed analysis experiments are also conducted to provide insights into performance across algorithms and comparisons between using C++ and Python, which can suggest directions for future studies.},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Quan, Shanghaoran and Yang, Jiaxi and Yu, Bowen and Zheng, Bo and Liu, Dayiheng and Yang, An and Ren, Xuancheng and Gao, Bofei and Miao, Yibo and Feng, Yunlong and Wang, Zekun and Yang, Jian and Cui, Zeyu and Fan, Yang and Zhang, Yichang and Hui, Binyuan and Lin, Junyang},
	month = jan,
	year = {2025},
	note = {arXiv:2501.01257 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\8PJV45BM\\Quan et al. - 2025 - CodeElo Benchmarking Competition-level Code Generation of LLMs with Human-comparable Elo Ratings.pdf:application/pdf},
}

@misc{liu_mathbench_2024,
	title = {{MathBench}: {Evaluating} the {Theory} and {Application} {Proficiency} of {LLMs} with a {Hierarchical} {Mathematics} {Benchmark}},
	shorttitle = {{MathBench}},
	url = {http://arxiv.org/abs/2405.12209},
	doi = {10.48550/arXiv.2405.12209},
	abstract = {Recent advancements in large language models (LLMs) have showcased significant improvements in mathematics. However, traditional math benchmarks like GSM8k offer a unidimensional perspective, falling short in providing a holistic assessment of the LLMs‚Äô math capabilities. To address this gap, we introduce MathBench, a new benchmark that rigorously assesses the mathematical capabilities of large language models. MathBench spans a wide range of mathematical disciplines, offering a detailed evaluation of both theoretical understanding and practical problem-solving skills. The benchmark progresses through five distinct stages, from basic arithmetic to college mathematics, and is structured to evaluate models at various depths of knowledge. Each stage includes theoretical questions and application problems, allowing us to measure a model‚Äôs mathematical proficiency and its ability to apply concepts in practical scenarios. MathBench aims to enhance the evaluation of LLMs‚Äô mathematical abilities, providing a nuanced view of their knowledge understanding levels and problem solving skills in a bilingual context. The project is released at https://github. com/open-compass/MathBench.},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Liu, Hongwei and Zheng, Zilong and Qiao, Yuxuan and Duan, Haodong and Fei, Zhiwei and Zhou, Fengzhe and Zhang, Wenwei and Zhang, Songyang and Lin, Dahua and Chen, Kai},
	month = may,
	year = {2024},
	note = {arXiv:2405.12209 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Project: https://github.com/open-compass/MathBench},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\CE2WJHK4\\Liu et al. - 2024 - MathBench Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	url = {http://arxiv.org/abs/2005.11401},
	doi = {10.48550/arXiv.2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when Ô¨Åne-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciÔ¨Åc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose Ô¨Åne-tuning recipe for retrieval-augmented generation (RAG) ‚Äî models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We Ô¨Åne-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciÔ¨Åc retrieve-and-extract architectures. For language generation tasks, we Ô¨Ånd that RAG models generate more speciÔ¨Åc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K√ºttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt√§schel, Tim and Riedel, Sebastian and Kiela, Douwe},
	month = apr,
	year = {2021},
	note = {arXiv:2005.11401 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted at NeurIPS 2020},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\FI7LM42T\\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}

@misc{ghareeb_robin_2025,
	title = {Robin: {A} multi-agent system for automating scientific discovery},
	shorttitle = {Robin},
	url = {http://arxiv.org/abs/2505.13400},
	doi = {10.48550/arXiv.2505.13400},
	abstract = {Scientific discovery is driven by the iterative process of background research, hypothesis generation, experimentation, and data analysis. Despite recent advancements in applying artificial intelligence to scientific discovery, no system has yet automated all of these stages in a single workflow. Here, we introduce Robin, the first multi-agent system capable of fully automating the key intellectual steps of the scientific process. By integrating literature search agents with data analysis agents, Robin can generate hypotheses, propose experiments, interpret experimental results, and generate updated hypotheses, achieving a semi-autonomous approach to scientific discovery. By applying this system, we were able to identify a novel treatment for dry age-related macular degeneration (dAMD), the major cause of blindness in the developed world. Robin proposed enhancing retinal pigment epithelium phagocytosis as a therapeutic strategy, and identified and validated a promising therapeutic candidate, ripasudil. Ripasudil is a clinically-used rho kinase (ROCK) inhibitor that has never previously been proposed for treating dAMD. To elucidate the mechanism of ripasudilinduced upregulation of phagocytosis, Robin then proposed and analyzed a follow-up RNA-seq experiment, which revealed upregulation of ABCA1, a critical lipid efflux pump and possible novel target. All hypotheses, experimental plans, data analyses, and data figures in the main text of this report were produced by Robin. As the first AI system to autonomously discover and validate a novel therapeutic candidate within an iterative lab-in-the-loop framework, Robin establishes a new paradigm for AI-driven scientific discovery.},
	language = {en},
	urldate = {2025-05-30},
	publisher = {arXiv},
	author = {Ghareeb, Ali Essam and Chang, Benjamin and Mitchener, Ludovico and Yiu, Angela and Szostkiewicz, Caralyn J. and Laurent, Jon M. and Razzak, Muhammed T. and White, Andrew D. and Hinks, Michaela M. and Rodriques, Samuel G.},
	month = may,
	year = {2025},
	note = {arXiv:2505.13400 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems, Quantitative Biology - Quantitative Methods},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\HUJZQIUQ\\Ghareeb et al. - 2025 - Robin A multi-agent system for automating scientific discovery.pdf:application/pdf},
}

@article{shi_large_nodate,
	title = {Large {Language} {Models} {Can} {Be} {Easily} {Distracted} by {Irrelevant} {Context}},
	language = {en},
	author = {Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed and Scharli, Nathanael and Zhou, Denny},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\MXHZKL6P\\Shi et al. - Large Language Models Can Be Easily Distracted by Irrelevant Context.pdf:application/pdf},
}

@misc{wu_autogen_2023,
	title = {{AutoGen}: {Enabling} {Next}-{Gen} {LLM} {Applications} via {Multi}-{Agent} {Conversation}},
	shorttitle = {{AutoGen}},
	url = {http://arxiv.org/abs/2308.08155},
	doi = {10.48550/arXiv.2308.08155},
	abstract = {AutoGen2 is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic framework for building diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.},
	language = {en},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Wu, Qingyun and Bansal, Gagan and Zhang, Jieyu and Wu, Yiran and Li, Beibin and Zhu, Erkang and Jiang, Li and Zhang, Xiaoyun and Zhang, Shaokun and Liu, Jiale and Awadallah, Ahmed Hassan and White, Ryen W. and Burger, Doug and Wang, Chi},
	month = oct,
	year = {2023},
	note = {arXiv:2308.08155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 43 pages (10 pages for the main text, 3 pages for references, and 30 pages for appendices)},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\DA2F4EDD\\Wu et al. - 2023 - AutoGen Enabling Next-Gen LLM Applications via Multi-Agent Conversation.pdf:application/pdf},
}

@misc{wang_agent_2024,
	title = {Agent {AI} with {LangGraph}: {A} {Modular} {Framework} for {Enhancing} {Machine} {Translation} {Using} {Large} {Language} {Models}},
	shorttitle = {Agent {AI} with {LangGraph}},
	url = {http://arxiv.org/abs/2412.03801},
	doi = {10.48550/arXiv.2412.03801},
	abstract = {This paper explores the transformative role of Agent AI and LangGraph in advancing the automation and effectiveness of machine translation (MT). Agents are modular components designed to perform specific tasks, such as translating between particular languages, with specializations like TranslateEnAgent, TranslateFrenchAgent, and TranslateJpAgent for English, French, and Japanese translations, respectively. These agents leverage the powerful semantic capabilities of large language models (LLMs), such as GPT-4o, to ensure accurate, contextually relevant translations while maintaining modularity, scalability, and context retention.},
	language = {en},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Wang, Jialin and Duan, Zhihua},
	month = dec,
	year = {2024},
	note = {arXiv:2412.03801 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\7ZWZZQ3I\\Wang and Duan - 2024 - Agent AI with LangGraph A Modular Framework for Enhancing Machine Translation Using Large Language.pdf:application/pdf},
}

@misc{noauthor_pricing_nodate,
	title = {Pricing},
	url = {https://openai.com/api/pricing/},
	abstract = {Simple and flexible. Only pay for what you use.},
	language = {en-US},
	urldate = {2025-06-26},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\IC99B895\\pricing.html:text/html},
}

@misc{noauthor_inspect_evalssrcinspect_evalslab_bench_nodate,
	title = {inspect\_evals/src/inspect\_evals/lab\_bench at main ¬∑ {UKGovernmentBEIS}/inspect\_evals},
	url = {https://github.com/UKGovernmentBEIS/inspect_evals/tree/main/src/inspect_evals/lab_bench},
	urldate = {2025-06-26},
	file = {inspect_evals/src/inspect_evals/lab_bench at main ¬∑ UKGovernmentBEIS/inspect_evals:C\:\\Users\\jackn\\Zotero\\storage\\TQNHNGK8\\lab_bench.html:text/html},
}

@misc{laurent_lab-bench_2024,
	title = {{LAB}-{Bench}: {Measuring} {Capabilities} of {Language} {Models} for {Biology} {Research}},
	shorttitle = {{LAB}-{Bench}},
	url = {http://arxiv.org/abs/2407.10362},
	doi = {10.48550/arXiv.2407.10362},
	abstract = {There is widespread optimism that frontier Large Language Models (LLMs) and LLM-augmented systems have the potential to rapidly accelerate scientific discovery across disciplines. Today, many benchmarks exist to measure LLM knowledge and reasoning on ‚Äútextbook‚Äù-style science questions, but few if any benchmarks are designed to evaluate language model performance on practical tasks required for scientific research, such as literature search, protocol planning, and data analysis. As a step toward building such benchmarks, we introduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of over 2,400 multiple choice questions for evaluating AI systems on a range of practical biology research capabilities, including recall and reasoning over literature, interpretation of figures, access and navigation of databases, and comprehension and manipulation of DNA and protein sequences. Importantly, in contrast to previous scientific benchmarks, we expect that an AI system that can achieve consistently high scores on the more difficult LAB-Bench tasks would serve as a useful assistant for researchers in areas such as literature search and molecular cloning. As an initial assessment of the emergent scientific task capabilities of frontier language models, we measure performance of several against our benchmark and report results compared to human expert biology researchers. We will continue to update and expand LAB-Bench over time, and expect it to serve as a useful tool in the development of automated research systems going forward. A public subset of LAB-Bench is available for use at the following url: https://huggingface.co/datasets/futurehouse/lab-bench.},
	language = {en},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Laurent, Jon M. and Janizek, Joseph D. and Ruzo, Michael and Hinks, Michaela M. and Hammerling, Michael J. and Narayanan, Siddharth and Ponnapati, Manvitha and White, Andrew D. and Rodriques, Samuel G.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10362 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 40 pages, 5 main figures, 1 main table, 2 supplemental figures, 4 supplemental tables. Submitted to NeurIPS 2024 Datasets and Benchmarks track (in review)},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\UATGASL6\\Laurent et al. - 2024 - LAB-Bench Measuring Capabilities of Language Models for Biology Research.pdf:application/pdf},
}

@misc{swyx_swyx_latentspacepod_2023,
	type = {Tweet},
	title = {@latentspacepod @{realGeorgeHotz} {GPT4} is 8 x {220B} params = 1.7 {Trillion} params https://t.co/{DW4jrzFEn2} ok {I} wasn't sure how widely to spread the rumors on {GPT}-4 but it seems {Soumith} is also confirming the same so here's the quick clip! so yes, {GPT4} is technically 10x the size of {GPT3}, and all the small https://t.co/{m2YiaHGVs4}},
	url = {https://x.com/swyx/status/1671272883379908608},
	language = {en},
	urldate = {2025-06-26},
	journal = {Twitter},
	author = {{swyx [@swyx]}},
	month = jun,
	year = {2023},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\4VWZTGWR\\1671272883379908608.html:text/html},
}

@misc{soumith_chintala_soumithchintala_i_2023,
	type = {Tweet},
	title = {i might have heard the same üòÉ -- {I} guess info like this is passed around but no one wants to say it out loud. {GPT}-4: 8 x {220B} experts trained with different data/task distributions and 16-iter inference. {Glad} that {Geohot} said it out loud. {Though}, at this point, {GPT}-4 is},
	url = {https://x.com/soumithchintala/status/1671267150101721090},
	language = {en},
	urldate = {2025-06-26},
	journal = {Twitter},
	author = {{Soumith Chintala [@soumithchintala]}},
	month = jun,
	year = {2023},
}

@misc{gwern_gpt-4_2023,
	type = {Reddit {Post}},
	title = {{GPT}-4 rumors: a {Mixture}-of-{Experts} w/8 {GPT}-3-220bs?},
	shorttitle = {{GPT}-4 rumors},
	url = {https://www.reddit.com/r/mlscaling/comments/14eowmw/gpt4_rumors_a_mixtureofexperts_w8_gpt3220bs/},
	urldate = {2025-06-26},
	journal = {r/mlscaling},
	author = {gwern},
	month = jun,
	year = {2023},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\JTQQ9YEG\\gpt4_rumors_a_mixtureofexperts_w8_gpt3220bs.html:text/html},
}

@misc{noauthor_non-determinism_2023,
	title = {Non-determinism in {GPT}-4 is caused by {Sparse} {MoE}},
	url = {https://152334H.github.io/blog/non-determinism-in-gpt-4/},
	abstract = {It‚Äôs well-known at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at temperature=0.0. This is an odd behavior if you‚Äôre used to dense decoder-only models, where temp=0 should imply greedy sampling which should imply full determinism, because the logits for the next token should be a pure function of the input sequence \& the model weights.},
	language = {en},
	urldate = {2025-06-26},
	journal = {152334H},
	month = aug,
	year = {2023},
	note = {Section: posts},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\54I56I9Q\\non-determinism-in-gpt-4.html:text/html},
}

@misc{noauthor_mixture_nodate,
	title = {Mixture of {Experts}: {Is} {GPT}-4 {Just} {Eight} {Smaller} {Models}?},
	shorttitle = {Mixture of {Experts}},
	url = {https://mattrickard.com/mixture-of-experts-is-gpt-4-just-eight-smaller-models},
	abstract = {In a recent interview, George Hotz [claimed](https://twitter.com/pommedeterre33/status/1671263789914677248) that GPT-4 is just an eight-way mixture model of 220},
	language = {en},
	urldate = {2025-06-26},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\ETGFRCG5\\mixture-of-experts-is-gpt-4-just-eight-smaller-models.html:text/html},
}

@misc{beconvincible_does_2023,
	type = {Reddit {Post}},
	title = {Does {ChatGPT} get better / worse at different times of the day? ({When} under load.)},
	shorttitle = {Does {ChatGPT} get better / worse at different times of the day?},
	url = {https://www.reddit.com/r/ChatGPTPro/comments/17lzq7b/does_chatgpt_get_better_worse_at_different_times/},
	urldate = {2025-06-26},
	journal = {r/ChatGPTPro},
	author = {BeConvincible},
	month = nov,
	year = {2023},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\8ESZU33J\\does_chatgpt_get_better_worse_at_different_times.html:text/html},
}

@misc{betts_peering_2023,
	title = {Peering {Inside} {GPT}-4: {Understanding} {Its} {Mixture} of {Experts} ({MoE}) {Architecture}},
	shorttitle = {Peering {Inside} {GPT}-4},
	url = {https://medium.com/@seanbetts/peering-inside-gpt-4-understanding-its-mixture-of-experts-moe-architecture-2a42eb8bdcb3},
	abstract = {Artificial Intelligence (AI) is rapidly evolving, breaking new ground, and bringing us closer than ever to Artifical General Intelligence‚Ä¶},
	language = {en},
	urldate = {2025-06-26},
	journal = {Medium},
	author = {Betts, Sean},
	month = jul,
	year = {2023},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\4EGLKDM9\\peering-inside-gpt-4-understanding-its-mixture-of-experts-moe-architecture-2a42eb8bdcb3.html:text/html},
}

@misc{noauthor_gpt-4o_nodate,
	title = {{GPT}-4o mini: advancing cost-efficient intelligence},
	shorttitle = {{GPT}-4o mini},
	url = {https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
	abstract = {Introducing the most cost-efficient small model in the market},
	language = {en-US},
	urldate = {2025-06-26},
}

@misc{xu_hallucination_2025,
	title = {Hallucination is {Inevitable}: {An} {Innate} {Limitation} of {Large} {Language} {Models}},
	shorttitle = {Hallucination is {Inevitable}},
	url = {http://arxiv.org/abs/2401.11817},
	doi = {10.48550/arXiv.2401.11817},
	abstract = {Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.},
	language = {en},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Xu, Ziwei and Jain, Sanjay and Kankanhalli, Mohan},
	month = feb,
	year = {2025},
	note = {arXiv:2401.11817 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\9EVLJ3M9\\Xu et al. - 2025 - Hallucination is Inevitable An Innate Limitation of Large Language Models.pdf:application/pdf},
}

@article{knight_openai_nodate,
	title = {{OpenAI} {Upgrades} {Its} {Smartest} {AI} {Model} {With} {Improved} {Reasoning} {Skills}},
	issn = {1059-1028},
	url = {https://www.wired.com/story/openai-o3-reasoning-model-google-gemini/},
	abstract = {A day after Google announced its first model capable of reasoning over problems, OpenAI has upped the stakes with an improved version of its own.},
	language = {en-US},
	urldate = {2025-06-26},
	journal = {Wired},
	author = {Knight, Will},
	note = {Section: tags},
	keywords = {artificial intelligence, chatbots, chatgpt, google, openai},
	file = {Snapshot:C\:\\Users\\jackn\\Zotero\\storage\\MUDIXL7Y\\openai-o3-reasoning-model-google-gemini.html:text/html},
}

@misc{han_llm_2025,
	title = {{LLM} {Multi}-{Agent} {Systems}: {Challenges} and {Open} {Problems}},
	shorttitle = {{LLM} {Multi}-{Agent} {Systems}},
	url = {http://arxiv.org/abs/2402.03578},
	doi = {10.48550/arXiv.2402.03578},
	abstract = {This paper explores multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex tasks through agent collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multiagent systems. We also explore potential applications of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.},
	language = {en},
	urldate = {2025-06-26},
	publisher = {arXiv},
	author = {Han, Shanshan and Zhang, Qifan and Yao, Yuhang and Jin, Weizhao and Xu, Zhaozhuo},
	month = may,
	year = {2025},
	note = {arXiv:2402.03578 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {PDF:C\:\\Users\\jackn\\Zotero\\storage\\JPC6LXDA\\Han et al. - 2025 - LLM Multi-Agent Systems Challenges and Open Problems.pdf:application/pdf},
}
