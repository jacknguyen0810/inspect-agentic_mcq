\section{Methods}
\label{sec:methods}

\subsection{Data ANalysis}
The first step was to collect and assemble the dataset for the LitQA benchmark. 
The questions, answers, and distractors, as well as the paper DOIs' were provided on FutureHouse's HuggingFace repository. 
Initially, two main issues were faced: only the training dataset was available, and that the papers themselves were not available. The test set was harder to access, but was eventually found. Then, with all of the DOIs, all of the individual paper PDFs were collected and assembled into the datasets. \\

The nexrt step was to analyse the papers themselves. During the collection of the papers, it was noticed that some of the dates did not meet the requirements stated by the paper authours. This was an issue because some of the results could then be affected and so the invalid papers were then identified, and it was found that 15 of the papers were from before the cutoff. FIGURE shows the years of the paper. These papers were excluded from the training dataset and a 'valid' dataset of pdfs was created. \\

The data from HuggingFace, which contained the question, correct answers and distractors, was turned into a \texttt{pandas} DataFrame. For every question, the correct answer and distractors were shuffled and then each assigned a letter to create a multiple choice question.

\subsection{Result Reproduction}
After the dataset was fixed, the next step was to reproduce the result using PaperQA.
The package has a very intuitive interface, the user uses the \texttt{ask} function (which is just a syncronous wrapper for the asynchonous function \texttt{agent\_query}), which then uses the prompt to find the relevant documents to answer the user's query. Each of the questions were fed to the \texttt{agent_query} and the asnwer was recorded.

The standard output of PaperQA is always a verbose summary of the answer, why the answer was chosen and citations from the paper that was used to answer the question. This is an extremely useful feature for scientists and researchers. 
However, this makes it difficult for us. We are looking for a single letter answer, so that it can easily be matched up with the corresponding correct result or distractors from the DataFrame. The initial thought was to use prompt engineering to try and force PaperQA to return the single correct letter. 

EXAMPLE PROMPT

This method did manage to create an output with a single letter, but the agent would always return a short summary explaining the reaosning. 
This led to very inconsistent results (where the position of the single letter answer within the text would vary), and made it difficult create a consistent evaluation pipline. \\

The solution to this issue was to make use of structured outputs. Structured outputs allow users to customise the exact format of the output of an LLM. However, PaperQA uses \texttt{LiteLLM} to call LLMs for tasks such as Paper Retrieval, Evidence Gathering, and Answer Generation. Structured Outputs formats vary between different LLMs and can only be used if the LLMs are called directly. PaperQA calls LLMs through a third-party \texttt{LiteLLM}, and so there is no 'entry point' for Structured Outputs to be used.

\subsection{Custom Multi-Agent Wrapper for PaperQA}
The solution to the above problem came in the same format as PaperQA itself, to make use of multi-agent systems. Structured Outputs using json style formatting was possible using AG2. 
Using AG2's Conversable Agent (the most basic agent), a rudimentary linear system was developed to format both the input and output of PaperQA to a standardised format. 
This allowed a conisistent output from PaperQA. 
FIGURE shows the strucutre of the agent. 
The need for a consistent format of the input was related to the evaluation methodology. 

\subsection{EvaluationL Insepct AI}
Inspect AI is a framework to evaluate the performance of LLMs by the UK's AI Securoty Institute. Inspect AI's \texttt{eval} tool allows for rapid performance evaluation by running LLM calls asynchonously. \texttt{eval} function consists of three parts: a dataset, a solver, and a scorer. The dataset is made up of \texttt{Samples}, which correspond to an individual task, or multiple choice question in this case. Each \texttt{Sample} contains fields such as inputs (which corresponds to our question), choices, and targets. The solver corresponds to the LLM system used to answer the question, and the scorer is the evaluation metric. \\

\texttt{inspect_evals} CITE is a custom library for evaluating various LLM's performances on benchamrks such as LitQA. However, it is only compatible with single LLMs, not multi-agent systems. \\

\texttt{inspect_ai} does offer support for multi-agent systems through the \texttt{bridge} function, but the package is new, and in its native form cannot support end-to-end MCQ evaluation with multi-agent systems. The \textt{bridge} function effectly allows multi-agents to replace the solver, as long as its output follows a specific format.
This format does not allow for information such as questions, choice, or target to be passed alonf in its native form, only allowing agent messages to be passed along. 
This prevents any information to the scorer the evaluation fails. \\

To this end, a custom package was built for multi-agent MCQ evaluation using \texttt{inspect_ai}. The 

