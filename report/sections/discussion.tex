\section{Discussion}
\label{sec:discussion}

We used the test set. Smaller, therefore more uncertainty in the result. Test set papers are all valid. Did not use full dataset due to cost and time budget. 

\subsection{Paper Methodology and Results}

\subsubsection{Human Benchmark}
The human benchmark defines whether LLM agents achieve superhuman performance. The criteria of the questions proposed seem suitable, with the requirements of being recently published and cutting-edge being mostly met. The majority of publications came from Nature and equivalent journals. Also, the fact that the answers to the questions cannot be inferred from the abstract or title should enforce the fact that the answers to the questions are not well known or trivial, requiring at least a basic level of reasoning to find the answers. Also, the fact that the incorrect options were distractors, often statistics or facts taken from the same paper and related statements (rather than, for exmaple, random numbers) means that the LLM agents and humans must truly understand the content, rather performing a simple search. Although this was the criteria set by the authours, the creation of the dataset was performed by external contractors, and as proved earlier, the dataset did not always meet these requirements (old, possibly well-known papers were included in the dataset). WIthout the time (and the biology expertise) to manually verify every question, this cast a doubt over the dataset, and hence both the human and the machine benchamrk. \\

In terms of the actual human performance, evalutors' academic backgrounds were not provided, although it might be possibly inferred as at least-univeristy level as the authours said that the evaluators had access to journals 'through their institutions'. Human evalutors were unrestricted, meaning they had access to the internet, journals and search engines, but not always the papers themselves. This implies that a large part of the task was actually finding the correct source for the question. IS THIS MIRRORED IN OUR RESULTS? LOOK AT THE DIFFERENCE IN EVIDENCE K and MAX SOURCES. Also, to maximise human performance, the evaluators were financially motivated to get as many questions correct as possible in a week. This makes the benchmark higher, which would try to emulate the maximum performance, which is approriate for testing if LLM agents are indeed superhuman. The evaluators were also told to not use generative search tools, but this was not enforaceble. Also, with financial incentive, there comes with a risk of cheating for financial gain. This risk seems fairly low, with the type of question making it very hard to cheat. So having faith in humanity, the human benchmark was accepted as valid. \\

\subsubsection{PaperQA Results}
The PaperQA result did have some major differences to the project experiemnts, some due to model differences, but also some due to general differences. \\

One difference was that the paper authors used custom HTTP environment to run paperqa. They used features such as MongoDB request caching, Redis object caching, global load-balancing, PostgreSQL DBs, custom cost monitoring, cloud storage and run orchestration. They said this did not affect per-run results, but increased scalability, measureability, and persistence. This highlighted a key difference in methodology, as this project was run locally on a single laptop, with average-at-best specs, local storage and local compute. Similar to the paper, the individual runs of PaperQA were always successful in testing, but running evaluations caused a lot of issues. Nearly of the errornous runs of this project was due to (wall-time) time-limit failures or LLM provider rate limits being hit. The time-limit limitation was due to the fact that the local compute would be having to run the \texttt{inspect\_ai} evals alongside PaperQA calls and this was often a more compute-intensive task. The rate error limts came from running multiple PaperQA, AG2 and Inspect AI calls in parallel, often from the same API key. All of these errors would affect the result, specifically, the accruacy metric. Errornous runs would be marked as NA, which would being down the accuracy, and increase the precision (less questions answered). The authors say their custom HTTPs setup would decrease these problems, which in turn may have skewed the results they achieved, had they ran this on a local machine as ordinary users would. 


\subsection{Project Results}

\subsubsection{Methodology Breakdown}
The methods employed in this project aimed to understand the source of and tackle any sources of bias and uncertainty in the original project. 

The first difference in methodology, was that our project and experiemnts were only ran on the test dataset, rather than the full LitQA benchmark quiz. This was for a multitide of reasons. Firstly, was budgetary: LLM agents cost money to run, and the budget did not allow for mulitple runs of the full LitQA dataset to be run. Secondly, as proven by the authours of the paper, 147 of the questions in the LitQA had been read by Google's data aggregator and made the questions invalid. However, we do know if these questions had been updated in the LitQA benchmark and more importantly, if these had been scraped by Google, they could have been scraped by OpenAI and other LLM providers o train the latest models, which defeates the purpose of testing these agents on their ability to synthesise new scientific information. \\

Another point of possible error was the use of AG2. AG2 also leverages LLMs for the formatting of the input/output. Therefore there was a risk of AG2 hallucinating, creating incorrect results. Although this was ahrd to confirm for every answer in every run, the AG2 structuring agents were tested thouroughly during prototyping and they returned the expected results every time. For all of the evaluation runs, this effect was checked for all of the questions, and all of the answers were valid. \\

The use of \texttt{inspect\_ai} was to follow standardised methods for evaluation. Since LLMs and LLM agents are such a new and rapidly evolving field, there is no standardised workflow for new systems. This meant that integrating PaperQA into AG2 into Inspect AI was extremely difficult. Inspect AI is also a very new tool, missing a lot of the functionailty required t evaluate multi-agent systems. Using Inspect AI has many benefits, such as running LLM calls asynchronously and in parallel, which theoretically should provide a speedup. However, there are also downsides. The eval tool prevents a lot of visibility and control over what is happening under the hood. It was also very difficult to pass along information along the \texttt{eval} pipeline, which facilitated the need to AG2 and also caused a lot of problems with cost tracking. 

\subsubsection{Hyperparameter Effects}
Evidence K and Max Sources should not make a huge effect on the answer, as all of the answers for each question come from a separate paper (no answers to a question come from multiple papers). But it does. THis is because the papers do not make up a single chunk. Instead, papers are often split up into multiple chunks. Also since all of the papers are biology, they are semantically similar, and so poor embeddings would create confusion and poor ranking. 

\subsubsection{Issues}
One very important issue noticed was the flucatuation in performance depending on LLM usage.
Noticed quite late on in the project, after performing a sizable number of runs, it appeared that the performance of LLMs would change depending on when the tests would run even with temperature 0. Looking on forums CITE, it appeared that this was a common problem, especially with OpenAI. Looking into the GPT-4 architecture could return a reason why this is happening. There is no GPT-4 techical paper that details its ful archtitecture due to competition, but leaks, expoert analysis and even confirmation by Sam Altman CITE point ot the fact that GPt-4 is a Mixture of Experts architecture. It is essentially an ensemble model of transformers and neural networks, each specialised for a 'expert' topic, rather than a general transformer, and the query is sent to a number of 'experts' to answer the question. The inference is then distributed to a number of computers and recollected for minimum computational load and maximum speed. However, it is this distributed factor that may cause the performance degradation at peak times. One reason is that even though we set temperature to 0, since the inference is distributed, it is not guaranteed that the same configuration is sent to each device (possibly due to compression etc.), and so floating point errors could accumulate and potentially lead to different outputs. Looking at MOE, it could be possible that at peak times, when resources are contested heavily, models could be degraded to ensure everyone has access to the models (e.g. instead of 3 experts you are only allocated 2). Also, to balance the load, the experts could be given less time for the current user's prompt, so that it is freed for the next user's prompt, resulting in less time for processing. These are all potential ideas why this is happening and is not confirmed by providers such as OpenAI. This points to possibly using less popular providers for better performance. 