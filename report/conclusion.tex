\section{Conclusion}
\label{sec:conclusion}
This report successfully reproduced and extended the evaluation of the PaperQA agent on the LitQA benchmark, revealing critical insights into the performance of modern agentic Retrieval-Augmented Generation (RAG) systems. The primary finding confirms the foundational premise of the original study: RAG systems significantly outperform standalone Large Language Models (LLMs), demonstrating superior precision by grounding responses in retrieved evidence and mitigating hallucination. Our experiments consistently achieved 'superhuman' performance that not only replicated but often exceeded the benchmarks set in the original paper, particularly when leveraging newer models like GPT-4o-mini and advanced text embeddings. \\

However, the investigation uncovered a more nuanced and complex reality. The results suggest a shift in the challenges of RAG systems; while the original paper emphasized the crucial role of the RCS step, our findings indicate that newer LLMs have diminished its importance. The primary performance bottleneck appears to have shifted from evidence re-ranking to the initial retrieval phase, where the quality of text embeddings and the inherent reasoning capabilities of the agent LLM are vital. \\

The most significant contribution of this work lies in highlighting the challenge of reproducibility. The performance of PaperQA was observed to be highly volatile, fluctuating based on API load during peak times—a phenomenon likely linked to the Mixture-of-Experts (MoE) architecture of models like GPT-4—and even exhibiting sensitivity to local hardware specifications. These external variables, which are outside the control of the user, introduce a troubling degree of variance and call into question whether these systems can be considered robustly superhuman if their performance cannot be consistently replicated.\\

Ultimately, while this project demonstrates that agentic RAG systems possess the raw capability to exceed human performance on specialized scientific question-answering tasks, it also serves as a warning on their practical readiness. The path to reliable deployment in scientific research depends not only on advancing the models themselves but also on establishing more stable infrastructure and transparent, standardized evaluation methodologies. Until then, 'superhuman' performance remains a brilliant but fragile achievement, heavily dependent on the specific implementation.\\

\section{Future Work}
The key issue is not whether language agents can achieve superhuman performance, but whether they can outperform humans consistently. The discussion talked about the implementation issues that caused flucatuations in performance (despite the agents being set to being determinisitic with 0 temperature). The best way to check this, would be to have an indication of peak times and to run the experiments again throughout these times, to get an indicator of average performance. \\

LitQA and PaperQA are biology-focused, but expanding the investigation to other scientific fields, where the process of science varies, could give a better indication of how good these RAG language agents are at performing science in general. Also, science is more than just sythesizing information. Science requires knowledge from disciplines such as mathematics, software engineering, and literature to create, test and publish new scientific hypotheses. Therefore, investigations into how Language agents can not only understand new information, but use it to write informed code and solve mathematical problems. Also, the creation of novel ideas is a topic that needs to be tested. \\