\section{Discussion}
\label{sec:discussion}
\subsection{Paper Methodology and Results}

\subsubsection{Human Benchmark}
The human benchmark is the standard used to determine whether LLM agents achieve superhuman performance. The criteria for the questions proposed in the LitQA benchmark appear suitable; the requirements for papers to be recently published and from cutting-edge fields were mostly met, with the majority of publications originating from Nature and journals of equivalent standing. The stipulation that answers cannot be inferred from the abstract or title enforces that the questions are non-trivial and require a degree of reasoning. Furthermore, the use of distractors—incorrect options derived from related facts or statistics from the same paper—necessitates a deep understanding of the content rather than a simple keyword search. However, despite these criteria set by the authors, the dataset was curated by external contractors and, as demonstrated in the previous section, did not always adhere to these requirements, including several papers that predated the cutoff. Without the resources to manually verify every question, the integrity of the dataset, and consequently both the human and machine benchmarks, is subject to a degree of uncertainty. \\

Regarding the human evaluators' performance, their specific academic backgrounds were not provided, although it can be inferred that they were at least at the university level, as the authors stated they had access to journals "through their institutions." The evaluators were unrestricted in their methodology, with access to the internet and search engines, but not necessarily direct access to the source papers. This implies that a significant part of their task was information retrieval—finding the correct source to answer the question. This mirrors the process undertaken by the PaperQA agent, where performance is heavily dependent on successfully retrieving the correct evidence chunks. Thus, the human benchmark closely resembles the task faced by the agent. \\

To maximize performance, the evaluators were financially incentivized to answer as many questions correctly as possible within a one-week period. This approach establishes a high-performance benchmark, appropriate for testing claims of superhuman capabilities. While evaluators were instructed not to use generative AI tools, this was not enforceable. The financial incentive also introduces a risk of cheating, though this risk is likely low given the specialized nature of the questions. Having faith in humanity, the human benchmark was deemed a valid, albeit imperfect, standard for comparison. \\

\subsubsection{Comparison with Original PaperQA Results}
The results of this project revealed some major differences when compared to the original paper's experiments, stemming from both model and methodological variations. \\

A significant methodological difference was the environment used. The original authors employed a custom HTTPS environment with features such as MongoDB request caching, Redis object caching, global load-balancing, and cloud-based orchestration. They stated this infrastructure did not affect per-run results but enhanced scalability, measurability, and persistence. In contrast, this project was executed on a local machine with standard specifications, storage, and compute. While individual PaperQA runs were successful, the full evaluation process encountered significant issues. The majority of erroneous runs were caused by wall-time limit failures or API rate-limiting. The time limit was often exceeded because the local machine had to manage the \texttt{inspect\_ai} evaluation framework concurrently with the agent calls, a compute-intensive process. Rate limit errors arose from running multiple agent calls in parallel from a single API key. These failures directly impacted the results; erroneous runs were marked as 'NA', which artificially lowered the accuracy score while potentially inflating the precision (as fewer questions were considered 'answered'). The authors' use of a custom infrastructure likely mitigated these issues, suggesting their reported results might not be representative of the performance an ordinary user would achieve on a local machine. \\


\subsection{Project Results}

\subsubsection{Methodology Breakdown}
The methods employed in this project aimed to identify and address potential sources of bias and uncertainty in the original study. \\

The first methodological difference was that our experiments were conducted exclusively on the test dataset, rather than the full LitQA benchmark. This decision was driven by several factors. Firstly, budgetary constraints prevented multiple runs on the full dataset. Secondly, the original authors had already identified that 147 questions in the benchmark had been compromised by Google's data aggregator. It is unknown if these questions have since been updated in the benchmark. More importantly, if this data was scraped by Google, it was likely also scraped by OpenAI and other providers to train their latest models, which defeats the purpose of testing the agents' ability to reason over novel scientific information. The use of a smaller evaluation dataset, however, introduces the inherent risk of higher variance in the results, as each question carries a greater weight in the final performance metrics. \\

Another potential source of error was the use of a secondary agent system for formatting. This is a common design pattern in multi-agent systems, where specialized agents handle tasks like I/O parsing. While this increases the robustness and consistency of the primary agent's output, it also introduces another component that could potentially fail or hallucinate, adding a layer of complexity to error analysis. \\

The use of \texttt{inspect\_ai} was intended to follow standardized evaluation methods. However, as LLMs and agentic systems represent a new and rapidly evolving field, a standardized workflow for integrating and evaluating such complex, multi-component systems does not yet exist. Integrating PaperQA into our custom agent system and then into Inspect AI proved to be extremely difficult. Inspect AI itself is a new tool and lacks some of the functionality required for streamlined multi-agent evaluation. While it offers benefits like asynchronous execution, which theoretically should provide a speedup, it also reduces visibility into the underlying processes. Passing metadata through the evaluation pipeline was particularly challenging, necessitating the multi-agent wrapper and creating difficulties with cost tracking. \\

Regarding asynchronous execution, while theoretically beneficial, running PaperQA calls in parallel proved to be counterproductive in practice. It significantly increased the time for individual queries (from ~1 minute to ~15 minutes for just two or three parallel queries) and led to a higher rate of timeout and rate-limit errors. This is likely because parallel requests rapidly exhaust the API's rate limit quota, triggering exponential backoff and retry mechanisms in the client library. This "retry storm" results in long wait times that ultimately make sequential execution both faster and more reliable. \\

\subsubsection{LLM Models and Embeddings}
Varying the LLMs had a less significant effect on precision than on accuracy. It appears that once a RAG system retrieves the correct context, most modern models are proficient at extracting the correct answer. All tested RAG configurations achieved 'superhuman' precision, underscoring the value of grounding models in external evidence. This contrasts sharply with the baseline \texttt{gpt-4o-mini} model without RAG, which attempted every question, leading to identical (and low) accuracy and precision scores due to frequent hallucination. With RAG, the models became 'aware' of their knowledge boundaries, preferring to abstain from answering rather than guessing, a critical feature for scientific applications. While no model achieved 100\% precision, the tendency to hallucinate was dramatically reduced compared to non-RAG performance. \\

The disparity between high variation in accuracy and low variation in precision suggests that the primary challenge for these systems is not reasoning over provided text, but successfully retrieving the correct text chunk in the first place. Once the relevant information was sourced, all models performed similarly well. This highlights that the most critical stage is information retrieval. \\

This conclusion is further supported by the strong performance of the configuration using Google's \texttt{text-embedding-004} model. Since all the papers in the LitQA corpus are from the field of biology, they are semantically similar, making it challenging for less advanced embedding models to distinguish between them. Better embeddings create more distinct clusters in the vector space for different nuanced topics. This reduces the "semantic noise" and helps the retriever pull more distinct and relevant chunks, leading to the highest accuracy observed. \\

A notable anomaly was the performance of \texttt{gpt-4.1}. Despite being a newer and more powerful model than \texttt{gpt-4-turbo}, it performed worse on this task, showing a greater hesitancy to answer and achieving lower accuracy. This suggests that general-purpose benchmark performance does not always translate to specialized, agentic RAG workflows and may indicate poorer performance in its function as either a retrieval or re-ranking agent. It is possible the model is over-optimized for conversational tasks at the expense of the kind of structured "tool-use" reasoning required by PaperQA. This behaviour remains unexplained, as the architectures of these proprietary models have not been released. Since the publication of the original paper, LLM research has advanced significantly, which is reflected in our results where nearly every RAG experiment achieved or exceeded human performance. \\

\subsubsection{Hyperparameter Effects}
The experimental results confirmed that system performance is most sensitive to hyperparameters controlling the retrieval process. \\

Increasing the answer cutoff (\texttt{max\_sources}) from 5 to 15 led to a decrease in both accuracy and precision. This contradicts the naive assumption that more context is always better. This phenomenon is likely an example of the "lost in the middle" problem CITE, where LLMs with large context windows struggle to attend to information buried in the middle of a long prompt. By expanding the number of sources, we increase the risk that the correct evidence, even if retrieved, is ignored by the model during the final synthesis step, leading it to answer incorrectly or hallucinate. \\

In the original paper, decreasing the top-k parameter led to a drastic reduction in accuracy. While our results reflected the same trend, the effect was far less pronounced. We found no clear correlation between the top-k value and accuracy, with newer models like \texttt{gpt-4o-mini} achieving near-superhuman performance even with a low top-k. This suggests that newer models are more effective in their initial retrieval step, identifying higher-quality, more relevant chunks from the outset. Consequently, the need for a large candidate pool for the subsequent RCS step is diminished. \\

This observation is further supported by the results from disabling the RCS step. In the original study, this was deemed critical for superhuman performance. In our experiments, its removal still yielded near-human performance, once again indicating that the improved baseline capabilities of newer models reduce the reliance on intensive re-ranking and filtering steps. \\

\subsubsection{Issues}
One very important issue noticed was the flucatuation in performance depending on LLM usage.
Noticed quite late on in the project, after performing a sizable number of runs, it appeared that the performance of LLMs would change depending on when the tests would run even with temperature 0. Looking on forums \cite{beconvincible_does_2023}, it appeared that this was a common problem, especially with OpenAI. Looking into the GPT-4 architecture could return a reason why this is happening. There is no GPT-4 techical paper that details its full archtitecture due to competition, but forums \cite{noauthor_mixture_nodate} \cite{gwern_gpt-4_2023}, leaks  and expert analysis \cite{soumith_chintala_soumithchintala_i_2023} \cite{swyx_swyx_latentspacepod_2023} \cite{betts_peering_2023} point to the fact that GPt-4 is a Mixture of Experts architecture. It is essentially an ensemble model of transformers and neural networks, each specialised for a 'expert' topic, rather than a general transformer, and the query is sent to a number of 'experts' to answer the question. The inference is then distributed to a number of computers and recollected for minimum computational load and maximum speed. However, it is this distributed factor that may cause the performance degradation at peak times. One reason is that even though we set temperature to 0, since the inference is distributed, it is not guaranteed that the same configuration is sent to each device (possibly due to compression etc.), and so floating point errors could accumulate and potentially lead to different outputs \cite{noauthor_non-determinism_2023}. Looking at MOE, it could be possible that at peak times, when resources are contested heavily, models could be degraded to ensure everyone has access to the models (e.g. instead of 3 experts you are only allocated 2, also known as graceful degradation). Also, to balance the load, the experts could be given less time for the current user's prompt, so that it is freed for the next user's prompt, resulting in less time for processing. These are all potential ideas why this is happening and is not confirmed by providers such as OpenAI. This points to possibly using less popular providers for better performance. \\

There seemed to also be a disparity between the effect of hardware on the performance of the runs. Due to technical issues, some of the runs had to be run on two different laptops. One laptop had below average hardware specifications (no GPU, Intel 12th Gen 5 Series CPU), and the other had top of the line specifications (e.g. top-end AMD GPU and CPU). Despite running the exact same code, and the same version for the packages, there was a slight difference in performance. Predictably, the better laptop completed the evaluation of the test set quicker, but the difference in time was substantial (usually 35 minutes, instead of 50 minutes). Considering the code is primarily API calls, this was peculiar, as there was no difference in code or packages, and there seemed to be no part of the code that relied heavily on CPU or GPU performance. \\

The difference in specs did not affect single PaperQA runs, but it did affect the performance of evaluation runs of the full test dataset. The faster runs also often came with higher accuracy and roughly equal precision. This reinforces the conclusion that system performance is sensitive to the entire end-to-end execution environment, not just the model itself. These external factors could also provide an alternative explanation for the unexpectedly poor performance of the GPT-4.1 model; it is possible those specific runs were affected by higher API latency. 
There seemed to be no plausible explanation for this, and the only way to confirm this would be to perform an focused, individual study instead. \\

These implementation issues could have explained the poorer performance of GPT 4.1 compared to GPT-4-Turbo, which was expected to perform better. The results were not errorneous, as every single question had a reasonable answer, with an appropriate reason behind why the answer was given.  \\


