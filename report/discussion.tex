\section{Discussion}
\label{sec:discussion}
\subsection{Paper Methodology and Results}

\subsubsection{Human Benchmark}
The human benchmark defines whether LLM agents achieve superhuman performance. The criteria of the questions proposed seem suitable, with the requirements of being recently published and cutting-edge being mostly met. The majority of publications came from Nature and equivalent journals. Also, the fact that the answers to the questions cannot be inferred from the abstract or title should enforce the fact that the answers to the questions are not well known or trivial, requiring at least a basic level of reasoning to find the answers. Also, the fact that the incorrect options were distractors, often statistics or facts taken from the same paper and related statements (rather than, for exmaple, random numbers) means that the LLM agents and humans must truly understand the content, rather performing a simple search. Although this was the criteria set by the authours, the creation of the dataset was performed by external contractors, and as proved earlier, the dataset did not always meet these requirements (old, possibly well-known papers were included in the dataset). WIthout the time (and the biology expertise) to manually verify every question, this cast a doubt over the dataset, and hence both the human and the machine benchamrk. \\

In terms of the actual human performance, evalutors' academic backgrounds were not provided, although it might be possibly inferred as at least-univeristy level as the authours said that the evaluators had access to journals 'through their institutions'. Human evalutors were unrestricted, meaning they had access to the internet, journals and search engines, but not always the papers themselves. This implies that a large part of the task was actually finding the correct source for the question. IS THIS MIRRORED IN OUR RESULTS? LOOK AT THE DIFFERENCE IN EVIDENCE K and MAX SOURCES. Also, to maximise human performance, the evaluators were financially motivated to get as many questions correct as possible in a week. This makes the benchmark higher, which would try to emulate the maximum performance, which is approriate for testing if LLM agents are indeed superhuman. The evaluators were also told to not use generative search tools, but this was not enforaceble. Also, with financial incentive, there comes with a risk of cheating for financial gain. This risk seems fairly low, with the type of question making it very hard to cheat. So having faith in humanity, the human benchmark was accepted as valid. \\

\subsubsection{PaperQA Results}
The PaperQA result did have some major differences to the project experiemnts, some due to model differences, but also some due to general differences. \\

One difference was that the paper authors used custom HTTP environment to run paperqa. They used features such as MongoDB request caching, Redis object caching, global load-balancing, PostgreSQL DBs, custom cost monitoring, cloud storage and run orchestration. They said this did not affect per-run results, but increased scalability, measureability, and persistence. This highlighted a key difference in methodology, as this project was run locally on a single laptop, with average-at-best specs, local storage and local compute. Similar to the paper, the individual runs of PaperQA were always successful in testing, but running evaluations caused a lot of issues. Nearly of the errornous runs of this project was due to (wall-time) time-limit failures or LLM provider rate limits being hit. The time-limit limitation was due to the fact that the local compute would be having to run the \texttt{inspect\_ai} evals alongside PaperQA calls and this was often a more compute-intensive task. The rate error limts came from running multiple PaperQA, AG2 and Inspect AI calls in parallel, often from the same API key. All of these errors would affect the result, specifically, the accruacy metric. Errornous runs would be marked as NA, which would being down the accuracy, and increase the precision (less questions answered). The authors say their custom HTTPs setup would decrease these problems, which in turn may have skewed the results they achieved, had they ran this on a local machine as ordinary users would. 


\subsection{Project Results}

\subsubsection{Methodology Breakdown}
The methods employed in this project aimed to understand the source of and tackle any sources of bias and uncertainty in the original project. 

The first difference in methodology, was that our project and experiemnts were only ran on the test dataset, rather than the full LitQA benchmark quiz. This was for a multitide of reasons. Firstly, was budgetary: LLM agents cost money to run, and the budget did not allow for mulitple runs of the full LitQA dataset to be run. Secondly, as proven by the authours of the paper, 147 of the questions in the LitQA had been read by Google's data aggregator and made the questions invalid. However, we do know if these questions had been updated in the LitQA benchmark and more importantly, if these had been scraped by Google, they could have been scraped by OpenAI and other LLM providers o train the latest models, which defeates the purpose of testing these agents on their ability to synthesise new scientific information. \\

Another point of possible error was the use of AG2. AG2 also leverages LLMs for the formatting of the input/output. Therefore there was a risk of AG2 hallucinating, creating incorrect results. Although this was ahrd to confirm for every answer in every evaluation run, the AG2 structuring agents were tested thouroughly during prototyping and they returned the expected results every time.\\

The use of \texttt{inspect\_ai} was to follow standardised methods for evaluation. Since LLMs and LLM agents are such a new and rapidly evolving field, there is no standardised workflow for new systems. This meant that integrating PaperQA into AG2 into Inspect AI was extremely difficult. Inspect AI is also a very new tool, missing a lot of the functionailty required to evaluate multi-agent systems. Using Inspect AI has many benefits, such as running LLM calls asynchronously and in parallel, which theoretically should provide a speedup. However, there are also downsides. The eval tool prevents a lot of visibility and control over what is happening under the hood. It was also very difficult to pass along information along the \texttt{eval} pipeline, which facilitated the need to AG2 and also caused a lot of problems with cost tracking.

On the topic of running LLM runs asynchronously; despite it being a good idea in theory, in practice running the PaperQA calls in parallel casued a lot of issues. Firstly, it would change the time taken for a a single PaperQA query from around a minute, to around 15 minutes. (This was only for two or three queries in parallel). This would also degrade the performance, as the user would begin to encounter timeout or rate limit errors, leading to loss of money, time and compute resources. In the end, the evaluation runs had to be made in series (one PaperQA call at a time), which actually sped up the code and improved performance. \\

\subsubsection{LLM Models}
Varying the LLMs did not have a considerable effect on the precision. It seems as though, all of the RAG models, achieved a precision (that used PaperQA over just the base model) achieved 'superhuamn' precision on question answer. Looking at the base model (\texttt{gpt-4o-mini} with no RAG), which achieved identical accuracy and precision, showed the importance of levaraging RAG systems. With no RAG, the model would attempt to answer every single question, and often hallucinating in the process. With RAG, the model because 'aware' of its limitation and the information it had available, which it would then be able to recognise that it was unable to, or had insufficient information to answer the question. \\

It appears that with RAG, the LLM was much less confident than just using a single LLM, preferring to opt for the safer choice of saying it does not know the answer rather than making the incorrect answer. This is a good sign, as it attempts to ground its answer in truth, making it ideal for scientific research. However, the precision was never 100\%, so it was still prone to hallucinating, but much less often comapared to non-RAG performance. 

It appeared that the individual LLM models behind PaperQA made little difference to the precision, but affected the accuracy more. This disparity in accuracy and precision suggests that if PaperQA was able to answer a question, it seemed to be much more confident and more likely to get the asnwer correct. It appears that the \textit{biggest challenge for these models was actually retrieving the correct chunk to find the answer}, and once it had recieved the appropriate chunk of text containing the asnwer, all of the models were fairly simialr in performance at understanding the text itself and extracting the required result. This exaplains the very large variations in accuracy and small variations in precision. 

TALK ABOUT CONTEXT WINDOWS AND HOW MINI 4.1 and TURBO compare, what it should be vs what it actually appeared in the results. 

Best Performance Gemini Embeddings. Better textual representation made the initial paper search step more effective. All of the papers are biology, they are semantically similar, and so poor embeddings would create confusion and poor ranking, but Gemeini embeddings allowed more nuance and difference to be picked up and helped identify more papers.



\subsubsection{Hyperparameter Effects}
As above, the biggest factor in performance is the retrieval step of PaperQA.
This is controlled by Top-K and Answer Cutoff.

ANSWER CUTOFF increase = POORER PERFORMANCE. WHY? 

TOP K REDUCED = MUCH LOWER ACCURACY. WHY? BUT INCREASING TOP K REDUCED ACCURACY. WHY?

Hvaing a reduced TOP K made a big impact on accuracy. MAKES SENSE AS LESS LIKELY TO FIND THE CORRECT CHUNK. 

NO RCS made a difference, but not as much as changing the top k and answer cutoff. SEEMS CRUCIAL FOR SUPERHUMAN PERFORMANCE. 

\subsubsection{Issues}
One very important issue noticed was the flucatuation in performance depending on LLM usage.
Noticed quite late on in the project, after performing a sizable number of runs, it appeared that the performance of LLMs would change depending on when the tests would run even with temperature 0. Looking on forums CITE, it appeared that this was a common problem, especially with OpenAI. Looking into the GPT-4 architecture could return a reason why this is happening. There is no GPT-4 techical paper that details its ful archtitecture due to competition, but leaks, expoert analysis and even confirmation by Sam Altman CITE point ot the fact that GPt-4 is a Mixture of Experts architecture. It is essentially an ensemble model of transformers and neural networks, each specialised for a 'expert' topic, rather than a general transformer, and the query is sent to a number of 'experts' to answer the question. The inference is then distributed to a number of computers and recollected for minimum computational load and maximum speed. However, it is this distributed factor that may cause the performance degradation at peak times. One reason is that even though we set temperature to 0, since the inference is distributed, it is not guaranteed that the same configuration is sent to each device (possibly due to compression etc.), and so floating point errors could accumulate and potentially lead to different outputs. Looking at MOE, it could be possible that at peak times, when resources are contested heavily, models could be degraded to ensure everyone has access to the models (e.g. instead of 3 experts you are only allocated 2). Also, to balance the load, the experts could be given less time for the current user's prompt, so that it is freed for the next user's prompt, resulting in less time for processing. These are all potential ideas why this is happening and is not confirmed by providers such as OpenAI. This points to possibly using less popular providers for better performance. 

There seemed to also be a disparity between the effect of hardware on the performance of the runs. Due to technical issues, some of the runs had to be run on two different laptops. One laptop had below average hardware specifications (no GPU, Intel 12th Gen 5 Series CPU), and the other had top of the line specifications (e.g. top-end AMD GPU and CPU). Despite running the exact same code, and the same version for the packages, there was a slight difference in performance. Predictably, the better laptop completed the evaluation of the test set quicker, but the difference in time was substantial (usually 35 minutes, instead of 50 minutes). Considering the code is primarily API calls, this was peculiar, as there was no difference in code or packages, and there seemed to be no part of the code that relied heavily on CPU or GPU performance. 
The difference in specs did not affect single PaperQA runs, but it did affect the performance of evaluation runs of the full test dataset. The faster runs also often came with higher accuracy and roughly equal precision. There seemed to be no plausible explanation for this, and the only way to confirm this would be to perform an focused, individual study instead. 

THESE ISSUES SEEM TO CAUSE ERRORNEOUS RUNS, ESPECIALLY FOR NO RCS, 4.1, 4-TURBO. High variation, and hard to find explanation for the high performance. 

\section{Future Work}

IF BUDGET ALLOWS: MORE RUNS, too high of variation. 

IDEALLY TRACK THE PEAK HOURS OF GPT AND THEN TEST AGAINST THESE HOURS TO HAVE MORE EVIDENCE OF THIS FLUCTIATION IN PERFORMANCE HAPPENING. THEN RESULTS COULD BE AVERAGED OUT IN A DAY TO GIVE AVERAGE PERFORMANCE. 


