{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking of LitQA2 (revised)\n",
    "\n",
    "Why? \n",
    "\n",
    "A new benchmark was developed for the LitQA2 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect_evals package\n",
    "\n",
    "Developed as a unified benchmark for LLM evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eval can be loaded in command line using: \n",
    "```\n",
    "inspect eval inspect_eval/lab_bench_litqa --model openai/gpt-4o-mini\n",
    "```\n",
    "\n",
    "From running the code in CLI, we can see that with gpt-4o-mini, we get an accuracy of 0.291, precision of 0.389, and coverage of 0.749. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we implement this into PaperQA2?\n",
    "\n",
    "We need to make a custom task for PaperQA2 to run. \n",
    "\n",
    "Develop the custom task using the test dataset and scale up to the full dataset. \n",
    "\n",
    "Breaking down the Task function, we have: \n",
    "- Dataset\n",
    "- Solver\n",
    "- Scorer\n",
    "- Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Libraries\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "# PaperQA2 Imports \n",
    "from paperqa import ask, Settings, agent_query\n",
    "from paperqa.settings import AgentSettings, AnswerSettings\n",
    "\n",
    "# Inspect AI Imports\n",
    "from inspect_ai import eval\n",
    "from inspect_ai import task, Task, Epochs\n",
    "from inspect_ai.dataset import MemoryDataset, json_dataset, FieldSpec, Sample\n",
    "from inspect_ai.solver._solver import solver, Solver, Generate\n",
    "from inspect_ai.solver._task_state import TaskState, ChatMessageUser#\n",
    "from inspect_ai.agent import bridge\n",
    "from inspect_ai.solver import _multiple_choice\n",
    "\n",
    "\n",
    "# Inspect Evals Imports\n",
    "from inspect_evals.lab_bench.record_to_sample_helpers import record_to_sample_base\n",
    "from inspect_evals.lab_bench.scorer import precision_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>ideal</th>\n",
       "      <th>distractors</th>\n",
       "      <th>canary</th>\n",
       "      <th>tag</th>\n",
       "      <th>version</th>\n",
       "      <th>sources</th>\n",
       "      <th>is_opensource</th>\n",
       "      <th>subtask</th>\n",
       "      <th>key-passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e6ece709-c919-4388-9f64-ab0e0822b03a</td>\n",
       "      <td>Approximately what percentage of topologically...</td>\n",
       "      <td>31%</td>\n",
       "      <td>[21%, 11%, 41%, 51%]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1038/s41467-024-44782-6]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>Good control in FPR does not necessarily repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813a9053-3f67-4d58-80af-02153de90ae4</td>\n",
       "      <td>At least how long do SynNotch-MCF10DCIS cells ...</td>\n",
       "      <td>72 h</td>\n",
       "      <td>[24, 48 h, 0 h, 12 h, 6 h, 96 h]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1073/pnas.2322688121]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>Spatial heterogeneity within tumors due to var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>831621de-5e32-4006-af84-a40dba100866</td>\n",
       "      <td>DK015 and DK038 strains of Verticillium dahlia...</td>\n",
       "      <td>95%</td>\n",
       "      <td>[94%, 96%, 97%, 98%]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1186/s12915-024-01900-6]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>The strains DK015 and DK038, with opposite MAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3e6d7a54-5b8a-4aa0-ac6e-1fce986d1636</td>\n",
       "      <td>Expression of which of the following genes was...</td>\n",
       "      <td>Aldh1l1</td>\n",
       "      <td>[MAPK, Actin, none of the above]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1073/pnas.2321711121]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>The mitogen-activated protein kinase (MAPK) pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4579ca5-c7d4-47a0-88f5-8adc460fc936</td>\n",
       "      <td>For which of the following Trub1 substrates di...</td>\n",
       "      <td>SCP2</td>\n",
       "      <td>[FBXO5, HECTD1, NKAIN1, CCDC22, IDI1]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1101/2024.03.26.586895]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>Among the Trub1 substrates, FBXO5 (chr6:152975...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  e6ece709-c919-4388-9f64-ab0e0822b03a   \n",
       "1  813a9053-3f67-4d58-80af-02153de90ae4   \n",
       "2  831621de-5e32-4006-af84-a40dba100866   \n",
       "3  3e6d7a54-5b8a-4aa0-ac6e-1fce986d1636   \n",
       "4  e4579ca5-c7d4-47a0-88f5-8adc460fc936   \n",
       "\n",
       "                                            question    ideal  \\\n",
       "0  Approximately what percentage of topologically...      31%   \n",
       "1  At least how long do SynNotch-MCF10DCIS cells ...     72 h   \n",
       "2  DK015 and DK038 strains of Verticillium dahlia...      95%   \n",
       "3  Expression of which of the following genes was...  Aldh1l1   \n",
       "4  For which of the following Trub1 substrates di...     SCP2   \n",
       "\n",
       "                             distractors  \\\n",
       "0                   [21%, 11%, 41%, 51%]   \n",
       "1       [24, 48 h, 0 h, 12 h, 6 h, 96 h]   \n",
       "2                   [94%, 96%, 97%, 98%]   \n",
       "3       [MAPK, Actin, none of the above]   \n",
       "4  [FBXO5, HECTD1, NKAIN1, CCDC22, IDI1]   \n",
       "\n",
       "                                              canary    tag  version  \\\n",
       "0  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "1  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "2  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "3  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "4  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "\n",
       "                                        sources  is_opensource        subtask  \\\n",
       "0  [https://doi.org/10.1038/s41467-024-44782-6]           True  litqa-v2-test   \n",
       "1     [https://doi.org/10.1073/pnas.2322688121]           True  litqa-v2-test   \n",
       "2  [https://doi.org/10.1186/s12915-024-01900-6]           True  litqa-v2-test   \n",
       "3     [https://doi.org/10.1073/pnas.2321711121]           True  litqa-v2-test   \n",
       "4   [https://doi.org/10.1101/2024.03.26.586895]           True  litqa-v2-test   \n",
       "\n",
       "                                         key-passage  \n",
       "0  Good control in FPR does not necessarily repre...  \n",
       "1  Spatial heterogeneity within tumors due to var...  \n",
       "2  The strains DK015 and DK038, with opposite MAT...  \n",
       "3  The mitogen-activated protein kinase (MAPK) pa...  \n",
       "4  Among the Trub1 substrates, FBXO5 (chr6:152975...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "litqa2_test_data = pd.read_parquet(\"/root/paperQA2_analysis/data/LitQA_data/test-00000-of-00001.parquet\")\n",
    "litqa2_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNCERTAIN_ANSWER_CHOICE = \"Insufficient information to answer the question.\"\n",
    "\n",
    "def record_to_sample_custom(record: dict) -> Sample:\n",
    "    # Preprocessing \n",
    "    choices = []\n",
    "    choices.append(record[\"ideal\"])\n",
    "    choices.extend(record[\"distractors\"])\n",
    "    choices.append(UNCERTAIN_ANSWER_CHOICE)\n",
    "    \n",
    "    return Sample(\n",
    "        input=record[\"question\"],\n",
    "        choices=choices,\n",
    "        target=\"A\"\n",
    "    )\n",
    "\n",
    "def convert_pandas_to_dataset(data: DataFrame) -> MemoryDataset:\n",
    "    records = data.to_dict(orient=\"records\")\n",
    "    samples = [record_to_sample_custom(i) for i in records]\n",
    "    \n",
    "    # Add to Dataset Object\n",
    "    dataset = MemoryDataset(samples)\n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input='Approximately what percentage of topologically associated domains in the GM12878 blood cell line does DiffDomain classify as reorganized in the K562 cell line?' choices=['31%', '21%', '11%', '41%', '51%', 'Insufficient information to answer the question.'] target='A' id=None metadata=None sandbox=None files=None setup=None\n",
      "input='At least how long do SynNotch-MCF10DCIS cells express BFP after contact with GFP+BMSC3 cells?' choices=['72 h', '24', '48 h', '0 h', '12 h', '6 h', '96 h', 'Insufficient information to answer the question.'] target='A' id=None metadata=None sandbox=None files=None setup=None\n"
     ]
    }
   ],
   "source": [
    "test_dataset = convert_pandas_to_dataset(litqa2_test_data)\n",
    "print(test_dataset.samples[0])\n",
    "print(test_dataset.samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM config (main LLM for reasoning, extract metadata, ...)\n",
    "llm_config_dict = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": \"gpt-4o-mini\",\n",
    "            \"litellm_params\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 4096\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"rate_limit\": {\"gpt-4o-mini\": \"30000 per 1 minute\"}\n",
    "}\n",
    "\n",
    "# Set up agent (answer search and selecting tools):\n",
    "agent_settings = AgentSettings(\n",
    "    agent_llm=\"gpt-4o-mini\",\n",
    "    agent_llm_config={\n",
    "        \"rate_limit\": \"30000 per 1 minute\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up summary LLM config\n",
    "summary_config_dict = {\n",
    "    \"rate_limit\": {\"gpt-4o-mini\": \"30000 per 1 minute\"}\n",
    "}\n",
    "\n",
    "# Set up answer format\n",
    "answer_settings = AnswerSettings(\n",
    "    evidence_k=30,\n",
    "    evidence_detailed_citations=False,\n",
    "    evidence_retrieval=False,\n",
    "    evidence_summary_length=\"around 100 words\",\n",
    "    evidence_skip_summary=False,\n",
    "    answer_max_sources=5,\n",
    "    max_answer_attempts=5,\n",
    "    answer_length=\"1 letter\"\n",
    ")\n",
    "\n",
    "# Set up the final settings object\n",
    "paperqa_settings = Settings(\n",
    "    llm=\"gpt-4o-mini\",\n",
    "    llm_config=llm_config_dict,\n",
    "    summary_llm=\"gpt-4o-mini\",\n",
    "    summary_llm_config=summary_config_dict,\n",
    "    agent=agent_settings,\n",
    "    temperature=0,\n",
    "    batch_size=1,\n",
    "    verbosity=1,\n",
    "    paper_directory=\"/root/paperQA2_analysis/data/LitQA_data/LitQA2_test_pdfs\"\n",
    ")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Templates\n",
    "# Create the templates for the questions?\n",
    "MULTIPLE_CHOICE_TEMPLATE = \"\"\"\n",
    "The following is a multiple choice question about biology.\n",
    "Please answer by responding with the letter of the correct answer.\n",
    "\n",
    "Think step by step.\n",
    "\n",
    "Question: {question}\n",
    "Options:\n",
    "{choices}\n",
    "\n",
    "You MUST include the letter of the correct answer within the following format: 'ANSWER: $LETTER' (without quotes). For example, ’ANSWER: <answer>’, where <answer> is the correct letter. Always answer in exactly this format of a single letter, even if you are unsure. We require this because we use automatic parsing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Solver should use the bridge function\n",
    "@solver\n",
    "def paperqa_solver(\n",
    "    template: str,\n",
    "    settings: dict,\n",
    "    shuffle: bool | None=None\n",
    ") -> Solver:\n",
    "    \n",
    "    # Async run PaperQA2 \n",
    "    async def solve(state: TaskState, generate: Generate):\n",
    "        print(state.user_prompt.text)\n",
    "        # Check choices available\n",
    "        if not state.choices:\n",
    "            raise ValueError(\"The multiple_choice solver requires samples with choices\")\n",
    "        \n",
    "        \n",
    "        # Format the prompt\n",
    "        prompt = _multiple_choice.prompt(\n",
    "            question=state.user_prompt.text,\n",
    "            choices=state.choices,\n",
    "            template=str(template)\n",
    "        )\n",
    "        \n",
    "        # Run paperQA2 with the template as the input\n",
    "        response = await agent_query(\n",
    "            query=prompt,\n",
    "            settings=settings\n",
    "        )\n",
    "        \n",
    "        print(\"response completed\")\n",
    "        print(type(response))\n",
    "        \n",
    "        # Update the state\n",
    "        state.messages.append(\n",
    "            ChatMessageUser(\n",
    "                content=response.completion\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # return await generate(state)\n",
    "        return state\n",
    "    \n",
    "    return solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini Example with 1 working sample\n",
    "example = {\n",
    "    \"question\": litqa2_test_data[\"question\"][0],\n",
    "    \"ideal\": litqa2_test_data[\"ideal\"][0],\n",
    "    \"distractors\": litqa2_test_data[\"distractors\"][0]\n",
    "}\n",
    "\n",
    "sample = record_to_sample_custom(example)\n",
    "mini_dataset = MemoryDataset([sample])\n",
    "# mini_dataset.shuffle_choices()\n",
    "\n",
    "@task\n",
    "def paperqa_eval_mini():\n",
    "    return Task(\n",
    "        dataset=mini_dataset,\n",
    "        solver = paperqa_solver(template=MULTIPLE_CHOICE_TEMPLATE, settings=paperqa_settings),\n",
    "        scorer=precision_choice(no_answer=UNCERTAIN_ANSWER_CHOICE),\n",
    "        epochs=Epochs(1, \"mode\")\n",
    "    )\n",
    "    \n",
    "nest_asyncio.apply()\n",
    "eval(paperqa_eval_mini())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "test_template = _multiple_choice.resource(MULTIPLE_CHOICE_TEMPLATE)\n",
    "\n",
    "test_sample = record_to_sample_custom(example)\n",
    "print(test_sample)\n",
    "test_prompt = MULTIPLE_CHOICE_TEMPLATE.format(\n",
    "    question=test_sample.input,\n",
    "    choices=test_sample.choices\n",
    ")\n",
    "\n",
    "# Test if the ask function is working correctly:\n",
    "test_repsonse = ask(\n",
    "    query=test_prompt,\n",
    "    settings=paperqa_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def paperqa_eval():\n",
    "    return Task(\n",
    "        dataset=test_dataset,\n",
    "        solver = bridge(paperqa_solver(template=MULTIPLE_CHOICE_TEMPLATE, settings=paperqa_settings)),\n",
    "        scorer=precision_choice(no_answer=UNCERTAIN_ANSWER_CHOICE),\n",
    "        epochs=Epochs(1, \"mode\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIPLE_CHOICE_TEMPLATE_CUSTOM = \"\"\"\n",
    "The following is a multiple choice question about biology.\n",
    "Please answer by responding with the letter of the correct answer.\n",
    "\n",
    "Think step by step.\n",
    "\n",
    "{question}\n",
    "\n",
    "You MUST include the letter of the correct answer within the following format: 'ANSWER: $LETTER' (without quotes). For example, ’ANSWER: <answer>’, where <answer> is the correct letter. Always answer in exactly this format of a single letter, even if you are unsure. We require this because we use automatic parsing.\n",
    "Strictly include none of your reasoning. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Record to Sample Custom\n",
    "def record_to_sample_custom(record: dict) -> Sample:\n",
    "    # Get the question\n",
    "    message = f\"Question: {record[\"question\"]} \\n\"\n",
    "    \n",
    "    # Concatenate the choices\n",
    "    choices = [record[\"ideal\"]]\n",
    "    choices.extend(record[\"distractors\"])\n",
    "    choices.append(UNCERTAIN_ANSWER_CHOICE)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    # Find the ideal answer\n",
    "    ideal_idx = choices.index(record[\"ideal\"])\n",
    "    \n",
    "    # Add prefixes to the shuffled choices\n",
    "    indices = list[range(len(choices))]\n",
    "    message +=  \"\\n\".join(\n",
    "        [f\"{chr(65 + i)}) {j}\" for i, j in enumerate(choices)]\n",
    "    )\n",
    "    \n",
    "    # Make the message a part of the Sample\n",
    "    return Sample(\n",
    "        input=message,\n",
    "        choices=choices,\n",
    "        target=f\"{chr(65 + ideal_idx)}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "# Preprocessing Code for Bridge Method\n",
    "def df_2_sample_bridge(data: DataFrame) -> MemoryDataset:\n",
    "    records = data.to_dict(orient=\"records\")\n",
    "    samples = [record_to_sample_custom(i) for i in records]\n",
    "    return MemoryDataset(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input='Approximately what percentage of topologically associated domains in the GM12878 blood cell line does DiffDomain classify as reorganized in the K562 cell line?' choices=['31%', '21%', '11%', '41%', '51%', 'Insufficient information to answer the question.'] target='A' id=None metadata=None sandbox=None files=None setup=None\n",
      "input='At least how long do SynNotch-MCF10DCIS cells express BFP after contact with GFP+BMSC3 cells?' choices=['72 h', '24', '48 h', '0 h', '12 h', '6 h', '96 h', 'Insufficient information to answer the question.'] target='A' id=None metadata=None sandbox=None files=None setup=None\n"
     ]
    }
   ],
   "source": [
    "test_dataset = df_2_sample_bridge(litqa2_test_data)\n",
    "print(test_dataset.samples[0])\n",
    "print(test_dataset.samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from autogen import LLMConfig, ConversableAgent\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class StructuredModel(BaseModel):\n",
    "    answer: str = Field(..., description=\"Answer, the single letter answer to the question, in the format of $LETTER\")\n",
    "    explanation: str = Field(..., description=\"Explanation, a short explanation of the answer with any citations found within the text.\")\n",
    "    citations: list[str] = Field(..., description=\"Citations, a list of citations found within the text.\")\n",
    "    \n",
    "    def format(self) -> str:\n",
    "        return f\"Answer: {self.answer}\\nExplanation: {self.explanation}\\nCitations: {self.citations}\"\n",
    "\n",
    "llm_config = LLMConfig(\n",
    "    api_type=\"openai\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    response_format=StructuredModel,\n",
    ")\n",
    "    \n",
    "    \n",
    "def structured_agent(output_text: str, structure: BaseModel, llm_config: LLMConfig):        \n",
    "    # Create the agent \n",
    "    agent = ConversableAgent(\n",
    "        name=\"structured_agent\",\n",
    "        llm_config=llm_config,\n",
    "        system_message=\"You are an agent that is able to parse the output of a given text and return the desired output.\",\n",
    "    )\n",
    "    \n",
    "    # Create the message\n",
    "    answer_message = \"\"\"\n",
    "    Please could you parse the following text and return the desired output.\n",
    "    \n",
    "    Text:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    answer_message = answer_message.format(text=output_text)\n",
    "    \n",
    "    \n",
    "    # Run the agent to take the output text and return the desired output.\n",
    "    response =agent.run(\n",
    "        message=answer_message,\n",
    "        max_turns=1,\n",
    "    )\n",
    "    \n",
    "    response.process()\n",
    "    \n",
    "    return response.output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "!echo $HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the output of the agent\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "test_text = \"\"\"Answer: DiffDomain identifies that approximately 30.771% of topologically associated domains (TADs) in  \n",
    "           the GM12878 blood cell line are reorganized in the K562 cell line                                       \n",
    "           (hua2024diffdomainenablesidentification pages 4-4). This finding is significant when compared to other  \n",
    "           methods, such as TADCompare, HiCcompare, and HiC-DC+, which only identify ≤8.256% of GM12878 TADs as    \n",
    "           reorganized in K562. The benchmarking results highlight the efficacy of DiffDomain in detecting         \n",
    "           reorganized TADs between these cell lines (hua2024diffdomainenablesidentification pages 4-4).           \n",
    "                                                                                                                   \n",
    "           Additionally, the analysis indicates that the majority of identified reorganized TADs have a minimum of \n",
    "           43.137%, a median of 81.357%, and a maximum of 98.022% represented by other subtypes                    \n",
    "           (hua2024diffdomainenablesidentification pages 4-5). This suggests a robust capability of DiffDomain in  \n",
    "           identifying reorganized TADs, establishing a notable extent of reorganization between GM12878 and K562  \n",
    "           (hua2024diffdomainenablesidentification pages 4-5).                                                     \n",
    "                                                                                                                   \n",
    "           In summary, the percentage of TADs in GM12878 classified as reorganized in K562 by DiffDomain is        \n",
    "           approximately 30.771%, which aligns with option E in the multiple-choice question.                      \n",
    "                                                                                                                   \n",
    "           ANSWER: E\"\"\"\n",
    "\n",
    "structured_answer = structured_agent(\n",
    "    output_text=test_text,\n",
    "    structure=StructuredModel,\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paperqa_agent(\n",
    "    template: str,\n",
    "    settings: dict,\n",
    "):\n",
    "    # Async run the solve:\n",
    "    async def run(sample: dict[str]) -> dict:\n",
    "        \n",
    "        # TODO: We need to run async here, but currently left as synchronous for testting\n",
    "        response = ask(\n",
    "            query=template.format(\n",
    "                question=sample[\"messages\"][0][\"content\"],\n",
    "            ),\n",
    "            settings=settings\n",
    "        )\n",
    "        \n",
    "        # TODO: Implement another AG2 agent to take the output of the first agent and return the desired output. \n",
    "        \n",
    "        return {\"output\": response.session.answer[-1]}\n",
    "    \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mini-version with 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini Example with 1 working sample\n",
    "example = {\n",
    "    \"question\": litqa2_test_data[\"question\"][0],\n",
    "    \"ideal\": litqa2_test_data[\"ideal\"][0],\n",
    "    \"distractors\": litqa2_test_data[\"distractors\"][0]\n",
    "}\n",
    "\n",
    "sample = record_to_sample_custom(example)\n",
    "mini_dataset = MemoryDataset([sample])\n",
    "# mini_dataset.shuffle_choices()\n",
    "\n",
    "@task\n",
    "def paperqa_eval_mini():\n",
    "    return Task(\n",
    "        dataset=mini_dataset,\n",
    "        solver = bridge(paperqa_agent(template=MULTIPLE_CHOICE_TEMPLATE_CUSTOM, settings=paperqa_settings)),\n",
    "        scorer=precision_choice(no_answer=UNCERTAIN_ANSWER_CHOICE),\n",
    "        epochs=Epochs(1, \"mode\")\n",
    "    )\n",
    "    \n",
    "# asyncio.run(eval(paperqa_eval_mini()))\n",
    "eval(paperqa_eval_mini())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_paperQA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
