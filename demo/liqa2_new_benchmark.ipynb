{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking of LitQA2 (revised)\n",
    "\n",
    "Why? \n",
    "\n",
    "A new benchmark was developed for the LitQA2 dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect_evals package\n",
    "\n",
    "Developed as a unified benchmark for LLM evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eval can be loaded in command line using: \n",
    "```\n",
    "inspect eval inspect_eval/lab_bench_litqa --model openai/gpt-4o-mini\n",
    "```\n",
    "\n",
    "From running the code in CLI, we can see that with gpt-4o-mini, we get an accuracy of 0.291, precision of 0.389, and coverage of 0.749. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we implement this into PaperQA2?\n",
    "\n",
    "We need to make a custom task for PaperQA2 to run. \n",
    "\n",
    "Develop the custom task using the test dataset and scale up to the full dataset. \n",
    "\n",
    "Breaking down the Task function, we have: \n",
    "- Dataset\n",
    "- Solver\n",
    "- Scorer\n",
    "- Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Libraries\n",
    "\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "# PaperQA2 Imports \n",
    "from paperqa import ask, Settings, agent_query\n",
    "from paperqa.settings import AgentSettings, AnswerSettings\n",
    "\n",
    "# Inspect AI Imports\n",
    "from inspect_ai import eval\n",
    "from inspect_ai import task, Task, Epochs\n",
    "from inspect_ai.dataset import MemoryDataset, json_dataset, FieldSpec, Sample\n",
    "from inspect_ai.solver._solver import solver, Solver, Generate\n",
    "from inspect_ai.solver._task_state import TaskState, ChatMessageUser\n",
    "from inspect_ai.agent import bridge\n",
    "from inspect_ai.solver import _multiple_choice\n",
    "from inspect_ai.scorer import Target, Scorer\n",
    "\n",
    "\n",
    "# Inspect Evals Imports\n",
    "from inspect_evals.lab_bench.record_to_sample_helpers import record_to_sample_base\n",
    "from inspect_evals.lab_bench.scorer import precision_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>ideal</th>\n",
       "      <th>distractors</th>\n",
       "      <th>canary</th>\n",
       "      <th>tag</th>\n",
       "      <th>version</th>\n",
       "      <th>sources</th>\n",
       "      <th>is_opensource</th>\n",
       "      <th>subtask</th>\n",
       "      <th>key-passage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e6ece709-c919-4388-9f64-ab0e0822b03a</td>\n",
       "      <td>Approximately what percentage of topologically...</td>\n",
       "      <td>31%</td>\n",
       "      <td>[21%, 11%, 41%, 51%]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1038/s41467-024-44782-6]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>Good control in FPR does not necessarily repre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>813a9053-3f67-4d58-80af-02153de90ae4</td>\n",
       "      <td>At least how long do SynNotch-MCF10DCIS cells ...</td>\n",
       "      <td>72 h</td>\n",
       "      <td>[24, 48 h, 0 h, 12 h, 6 h, 96 h]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1073/pnas.2322688121]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>Spatial heterogeneity within tumors due to var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>831621de-5e32-4006-af84-a40dba100866</td>\n",
       "      <td>DK015 and DK038 strains of Verticillium dahlia...</td>\n",
       "      <td>95%</td>\n",
       "      <td>[94%, 96%, 97%, 98%]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1186/s12915-024-01900-6]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>The strains DK015 and DK038, with opposite MAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3e6d7a54-5b8a-4aa0-ac6e-1fce986d1636</td>\n",
       "      <td>Expression of which of the following genes was...</td>\n",
       "      <td>Aldh1l1</td>\n",
       "      <td>[MAPK, Actin, none of the above]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1073/pnas.2321711121]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>The mitogen-activated protein kinase (MAPK) pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e4579ca5-c7d4-47a0-88f5-8adc460fc936</td>\n",
       "      <td>For which of the following Trub1 substrates di...</td>\n",
       "      <td>SCP2</td>\n",
       "      <td>[FBXO5, HECTD1, NKAIN1, CCDC22, IDI1]</td>\n",
       "      <td>BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...</td>\n",
       "      <td>litqa</td>\n",
       "      <td>1.1-dev</td>\n",
       "      <td>[https://doi.org/10.1101/2024.03.26.586895]</td>\n",
       "      <td>True</td>\n",
       "      <td>litqa-v2-test</td>\n",
       "      <td>Among the Trub1 substrates, FBXO5 (chr6:152975...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  e6ece709-c919-4388-9f64-ab0e0822b03a   \n",
       "1  813a9053-3f67-4d58-80af-02153de90ae4   \n",
       "2  831621de-5e32-4006-af84-a40dba100866   \n",
       "3  3e6d7a54-5b8a-4aa0-ac6e-1fce986d1636   \n",
       "4  e4579ca5-c7d4-47a0-88f5-8adc460fc936   \n",
       "\n",
       "                                            question    ideal  \\\n",
       "0  Approximately what percentage of topologically...      31%   \n",
       "1  At least how long do SynNotch-MCF10DCIS cells ...     72 h   \n",
       "2  DK015 and DK038 strains of Verticillium dahlia...      95%   \n",
       "3  Expression of which of the following genes was...  Aldh1l1   \n",
       "4  For which of the following Trub1 substrates di...     SCP2   \n",
       "\n",
       "                             distractors  \\\n",
       "0                   [21%, 11%, 41%, 51%]   \n",
       "1       [24, 48 h, 0 h, 12 h, 6 h, 96 h]   \n",
       "2                   [94%, 96%, 97%, 98%]   \n",
       "3       [MAPK, Actin, none of the above]   \n",
       "4  [FBXO5, HECTD1, NKAIN1, CCDC22, IDI1]   \n",
       "\n",
       "                                              canary    tag  version  \\\n",
       "0  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "1  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "2  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "3  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "4  BENCHMARK DATA SHOULD NEVER APPEAR IN TRAINING...  litqa  1.1-dev   \n",
       "\n",
       "                                        sources  is_opensource        subtask  \\\n",
       "0  [https://doi.org/10.1038/s41467-024-44782-6]           True  litqa-v2-test   \n",
       "1     [https://doi.org/10.1073/pnas.2322688121]           True  litqa-v2-test   \n",
       "2  [https://doi.org/10.1186/s12915-024-01900-6]           True  litqa-v2-test   \n",
       "3     [https://doi.org/10.1073/pnas.2321711121]           True  litqa-v2-test   \n",
       "4   [https://doi.org/10.1101/2024.03.26.586895]           True  litqa-v2-test   \n",
       "\n",
       "                                         key-passage  \n",
       "0  Good control in FPR does not necessarily repre...  \n",
       "1  Spatial heterogeneity within tumors due to var...  \n",
       "2  The strains DK015 and DK038, with opposite MAT...  \n",
       "3  The mitogen-activated protein kinase (MAPK) pa...  \n",
       "4  Among the Trub1 substrates, FBXO5 (chr6:152975...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "litqa2_test_data = pd.read_parquet(\"/root/paperQA2_analysis/data/LitQA_data/test-00000-of-00001.parquet\")\n",
    "litqa2_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNCERTAIN_ANSWER_CHOICE = \"Insufficient information to answer the question.\"\n",
    "\n",
    "def record_to_sample_custom(record: dict) -> Sample:\n",
    "    # Preprocessing \n",
    "    choices = []\n",
    "    choices.append(record[\"ideal\"])\n",
    "    choices.extend(record[\"distractors\"])\n",
    "    choices.append(UNCERTAIN_ANSWER_CHOICE)\n",
    "    \n",
    "    return Sample(\n",
    "        input=record[\"question\"],\n",
    "        choices=choices,\n",
    "        target=\"A\"\n",
    "    )\n",
    "\n",
    "def convert_pandas_to_dataset(data: DataFrame) -> MemoryDataset:\n",
    "    records = data.to_dict(orient=\"records\")\n",
    "    samples = [record_to_sample_custom(i) for i in records]\n",
    "    \n",
    "    # Add to Dataset Object\n",
    "    dataset = MemoryDataset(samples)\n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM config (main LLM for reasoning, extract metadata, ...)\n",
    "llm_config_dict = {\n",
    "    \"model_list\": [\n",
    "        {\n",
    "            \"model_name\": \"gpt-4o-mini\",\n",
    "            \"litellm_params\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"temperature\": 0,\n",
    "                \"max_tokens\": 4096\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"rate_limit\": {\"gpt-4o-mini\": \"30000 per 1 minute\"}\n",
    "}\n",
    "\n",
    "# Set up agent (answer search and selecting tools):\n",
    "agent_settings = AgentSettings(\n",
    "    agent_llm=\"gpt-4o-mini\",\n",
    "    agent_llm_config={\n",
    "        \"rate_limit\": \"30000 per 1 minute\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up summary LLM config\n",
    "summary_config_dict = {\n",
    "    \"rate_limit\": {\"gpt-4o-mini\": \"30000 per 1 minute\"}\n",
    "}\n",
    "\n",
    "# Set up answer format\n",
    "answer_settings = AnswerSettings(\n",
    "    evidence_k=30,\n",
    "    evidence_detailed_citations=False,\n",
    "    evidence_retrieval=False,\n",
    "    evidence_summary_length=\"around 100 words\",\n",
    "    evidence_skip_summary=False,\n",
    "    answer_max_sources=5,\n",
    "    max_answer_attempts=5,\n",
    "    answer_length=\"1 letter\"\n",
    ")\n",
    "\n",
    "# Set up the final settings object\n",
    "paperqa_settings = Settings(\n",
    "    llm=\"gpt-4o-mini\",\n",
    "    llm_config=llm_config_dict,\n",
    "    summary_llm=\"gpt-4o-mini\",\n",
    "    summary_llm_config=summary_config_dict,\n",
    "    agent=agent_settings,\n",
    "    temperature=0,\n",
    "    batch_size=1,\n",
    "    verbosity=1,\n",
    "    paper_directory=\"/root/paperQA2_analysis/data/LitQA_data/LitQA2_test_pdfs\"\n",
    ")      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer Templates\n",
    "# Create the templates for the questions?\n",
    "MULTIPLE_CHOICE_TEMPLATE = \"\"\"\n",
    "The following is a multiple choice question about biology.\n",
    "Please answer by responding with the letter of the correct answer.\n",
    "\n",
    "Think step by step.\n",
    "\n",
    "Question: {question}\n",
    "Options:\n",
    "{choices}\n",
    "\n",
    "You MUST include the letter of the correct answer within the following format: 'ANSWER: $LETTER' (without quotes). For example, ’ANSWER: <answer>’, where <answer> is the correct letter. Always answer in exactly this format of a single letter, even if you are unsure. We require this because we use automatic parsing.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def paperqa_eval():\n",
    "    return Task(\n",
    "        dataset=test_dataset,\n",
    "        solver = bridge(paperqa_solver(template=MULTIPLE_CHOICE_TEMPLATE, settings=paperqa_settings)),\n",
    "        scorer=precision_choice(no_answer=UNCERTAIN_ANSWER_CHOICE),\n",
    "        epochs=Epochs(1, \"mode\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIPLE_CHOICE_TEMPLATE_CUSTOM = \"\"\"\n",
    "The following is a multiple choice question about biology.\n",
    "Please answer by responding with the letter of the correct answer.\n",
    "\n",
    "Think step by step.\n",
    "\n",
    "{question}\n",
    "\n",
    "Return your answer in the following format:\n",
    "\n",
    "\"letter\".\n",
    "\n",
    "where the letter denotes your chosen answer from the available options. You MUST only include the letter (with no quotation marks) and NOTHING ELSE.\n",
    "\"\"\"\n",
    "\n",
    "UNCERTAIN_ANSWER_CHOICE = \"Insufficient information to answer the question.\"\n",
    "\n",
    "\n",
    "# You MUST include the letter of the correct answer within the following format: 'ANSWER: $LETTER' (without quotes). For example, ’ANSWER: <answer>’, where <answer> is the correct letter. Always answer in exactly this format of a single letter, even if you are unsure. We require this because we use automatic parsing. Include your reasoning and context after this, separated by a line.  \n",
    "\n",
    "\n",
    "# Record to Sample Custom\n",
    "def record_to_sample_custom(record: dict) -> Sample:\n",
    "    # Get the question\n",
    "    message = f\"Question: {record[\"question\"]} \\n\"\n",
    "    \n",
    "    # Concatenate the choices\n",
    "    choices = [record[\"ideal\"]]\n",
    "    choices.extend(record[\"distractors\"])\n",
    "    choices.append(UNCERTAIN_ANSWER_CHOICE)\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    random.shuffle(choices)\n",
    "    \n",
    "    # Find the ideal answer\n",
    "    ideal_idx = choices.index(record[\"ideal\"])\n",
    "    \n",
    "    # Add prefixes to the shuffled choices\n",
    "    indices = list[range(len(choices))]\n",
    "    message +=  \"\\n\".join(\n",
    "        [f\"{chr(65 + i)}) {j}\" for i, j in enumerate(choices)]\n",
    "    )\n",
    "    \n",
    "    # Make the message a part of the Sample\n",
    "    return Sample(\n",
    "        input=message,\n",
    "        choices=choices,\n",
    "        target=f\"{chr(65 + ideal_idx)}\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "# Preprocessing Code for Bridge Method\n",
    "def df_2_sample_bridge(data: DataFrame) -> MemoryDataset:\n",
    "    records = data.to_dict(orient=\"records\")\n",
    "    samples = [record_to_sample_custom(i) for i in records]\n",
    "    return MemoryDataset(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input='Question: Approximately what percentage of topologically associated domains in the GM12878 blood cell line does DiffDomain classify as reorganized in the K562 cell line? \\nA) 21%\\nB) 11%\\nC) 41%\\nD) 31%\\nE) Insufficient information to answer the question.\\nF) 51%' choices=['21%', '11%', '41%', '31%', 'Insufficient information to answer the question.', '51%'] target='D' id=None metadata=None sandbox=None files=None setup=None\n",
      "input='Question: At least how long do SynNotch-MCF10DCIS cells express BFP after contact with GFP+BMSC3 cells? \\nA) 72 h\\nB) 0 h\\nC) 6 h\\nD) 96 h\\nE) Insufficient information to answer the question.\\nF) 48 h\\nG) 12 h\\nH) 24' choices=['72 h', '0 h', '6 h', '96 h', 'Insufficient information to answer the question.', '48 h', '12 h', '24'] target='A' id=None metadata=None sandbox=None files=None setup=None\n"
     ]
    }
   ],
   "source": [
    "test_dataset = df_2_sample_bridge(litqa2_test_data)\n",
    "print(test_dataset.samples[0])\n",
    "print(test_dataset.samples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paperqa_agent(\n",
    "    template: str,\n",
    "    settings: dict,\n",
    "):\n",
    "    # Async run the solve:\n",
    "    async def run(sample: dict[str]) -> dict:\n",
    "        # print(sample[\"messages\"][0][\"content\"])\n",
    "        \n",
    "        response = ask(\n",
    "            query=template.format(\n",
    "                question=sample[\"messages\"][0][\"content\"],\n",
    "            ),\n",
    "            settings=settings\n",
    "        )\n",
    "        \n",
    "        return {\"output\": response.session.answer}\n",
    "    \n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mini-version with 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Task(\n\u001b[1;32m     17\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mmini_dataset,\n\u001b[1;32m     18\u001b[0m         solver \u001b[38;5;241m=\u001b[39m bridge(paperqa_agent(template\u001b[38;5;241m=\u001b[39mMULTIPLE_CHOICE_TEMPLATE_CUSTOM, settings\u001b[38;5;241m=\u001b[39mpaperqa_settings)),\n\u001b[1;32m     19\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mpaperqa_scorer(no_answer\u001b[38;5;241m=\u001b[39mUNCERTAIN_ANSWER_CHOICE),\n\u001b[1;32m     20\u001b[0m         epochs\u001b[38;5;241m=\u001b[39mEpochs(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# asyncio.run(eval(paperqa_eval_mini()))\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaperqa_eval_mini\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/_eval/eval.py:248\u001b[0m, in \u001b[0;36meval\u001b[0;34m(tasks, model, model_base_url, model_args, model_roles, task_args, sandbox, sandbox_cleanup, solver, tags, metadata, trace, display, approval, log_level, log_level_transcript, log_dir, log_format, limit, sample_id, epochs, fail_on_error, retry_on_error, debug_errors, message_limit, token_limit, time_limit, working_limit, max_samples, max_tasks, max_subprocesses, max_sandboxes, log_samples, log_realtime, log_images, log_buffer, log_shared, score, score_display, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_task_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_task_app\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/_display/rich/display.py:80\u001b[0m, in \u001b[0;36mRichDisplay.run_task_app\u001b[0;34m(self, main)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_task_app\u001b[39m(\u001b[38;5;28mself\u001b[39m, main: Callable[[], Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, TR]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TR:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m running_in_notebook():\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_coroutine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m anyio\u001b[38;5;241m.\u001b[39mrun(main, backend\u001b[38;5;241m=\u001b[39mconfigured_async_backend())\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/_util/_async.py:116\u001b[0m, in \u001b[0;36mrun_coroutine\u001b[0;34m(coroutine)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m running_in_notebook():\n\u001b[1;32m    115\u001b[0m     init_nest_asyncio()\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;66;03m# this will throw if there is no running loop\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/_eval/eval.py:201\u001b[0m, in \u001b[0;36meval.<locals>.run_task_app\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_task_app\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[EvalLog]:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m eval_async(\n\u001b[1;32m    202\u001b[0m             tasks\u001b[38;5;241m=\u001b[39mtasks,\n\u001b[1;32m    203\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    204\u001b[0m             model_base_url\u001b[38;5;241m=\u001b[39mmodel_base_url,\n\u001b[1;32m    205\u001b[0m             model_args\u001b[38;5;241m=\u001b[39mmodel_args,\n\u001b[1;32m    206\u001b[0m             model_roles\u001b[38;5;241m=\u001b[39mmodel_roles,\n\u001b[1;32m    207\u001b[0m             task_args\u001b[38;5;241m=\u001b[39mtask_args,\n\u001b[1;32m    208\u001b[0m             sandbox\u001b[38;5;241m=\u001b[39msandbox,\n\u001b[1;32m    209\u001b[0m             sandbox_cleanup\u001b[38;5;241m=\u001b[39msandbox_cleanup,\n\u001b[1;32m    210\u001b[0m             solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[1;32m    211\u001b[0m             tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    212\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    213\u001b[0m             approval\u001b[38;5;241m=\u001b[39mapproval,\n\u001b[1;32m    214\u001b[0m             log_level\u001b[38;5;241m=\u001b[39mlog_level,\n\u001b[1;32m    215\u001b[0m             log_level_transcript\u001b[38;5;241m=\u001b[39mlog_level_transcript,\n\u001b[1;32m    216\u001b[0m             log_dir\u001b[38;5;241m=\u001b[39mlog_dir,\n\u001b[1;32m    217\u001b[0m             log_format\u001b[38;5;241m=\u001b[39mlog_format,\n\u001b[1;32m    218\u001b[0m             limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[1;32m    219\u001b[0m             sample_id\u001b[38;5;241m=\u001b[39msample_id,\n\u001b[1;32m    220\u001b[0m             epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    221\u001b[0m             fail_on_error\u001b[38;5;241m=\u001b[39mfail_on_error,\n\u001b[1;32m    222\u001b[0m             retry_on_error\u001b[38;5;241m=\u001b[39mretry_on_error,\n\u001b[1;32m    223\u001b[0m             debug_errors\u001b[38;5;241m=\u001b[39mdebug_errors,\n\u001b[1;32m    224\u001b[0m             message_limit\u001b[38;5;241m=\u001b[39mmessage_limit,\n\u001b[1;32m    225\u001b[0m             token_limit\u001b[38;5;241m=\u001b[39mtoken_limit,\n\u001b[1;32m    226\u001b[0m             time_limit\u001b[38;5;241m=\u001b[39mtime_limit,\n\u001b[1;32m    227\u001b[0m             working_limit\u001b[38;5;241m=\u001b[39mworking_limit,\n\u001b[1;32m    228\u001b[0m             max_samples\u001b[38;5;241m=\u001b[39mmax_samples,\n\u001b[1;32m    229\u001b[0m             max_tasks\u001b[38;5;241m=\u001b[39mmax_tasks,\n\u001b[1;32m    230\u001b[0m             max_subprocesses\u001b[38;5;241m=\u001b[39mmax_subprocesses,\n\u001b[1;32m    231\u001b[0m             max_sandboxes\u001b[38;5;241m=\u001b[39mmax_sandboxes,\n\u001b[1;32m    232\u001b[0m             log_samples\u001b[38;5;241m=\u001b[39mlog_samples,\n\u001b[1;32m    233\u001b[0m             log_realtime\u001b[38;5;241m=\u001b[39mlog_realtime,\n\u001b[1;32m    234\u001b[0m             log_images\u001b[38;5;241m=\u001b[39mlog_images,\n\u001b[1;32m    235\u001b[0m             log_buffer\u001b[38;5;241m=\u001b[39mlog_buffer,\n\u001b[1;32m    236\u001b[0m             log_shared\u001b[38;5;241m=\u001b[39mlog_shared,\n\u001b[1;32m    237\u001b[0m             score\u001b[38;5;241m=\u001b[39mscore,\n\u001b[1;32m    238\u001b[0m             score_display\u001b[38;5;241m=\u001b[39mscore_display,\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# exceptions can escape when debug_errors is True and that's okay\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ExceptionGroup \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/_eval/eval.py:503\u001b[0m, in \u001b[0;36meval_async\u001b[0;34m(tasks, model, model_base_url, model_args, model_roles, task_args, sandbox, sandbox_cleanup, solver, tags, metadata, approval, log_level, log_level_transcript, log_dir, log_format, limit, sample_id, epochs, fail_on_error, retry_on_error, debug_errors, message_limit, token_limit, time_limit, working_limit, max_samples, max_tasks, max_subprocesses, max_sandboxes, log_samples, log_realtime, log_images, log_buffer, log_shared, score, score_display, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, task_definitions):\n\u001b[1;32m    499\u001b[0m     task_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m t: t\u001b[38;5;241m.\u001b[39msequence \u001b[38;5;241m==\u001b[39m sequence, resolved_tasks)\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m eval_run(\n\u001b[1;32m    504\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[1;32m    505\u001b[0m             tasks\u001b[38;5;241m=\u001b[39mtask_batch,\n\u001b[1;32m    506\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[1;32m    507\u001b[0m             eval_config\u001b[38;5;241m=\u001b[39meval_config,\n\u001b[1;32m    508\u001b[0m             eval_sandbox\u001b[38;5;241m=\u001b[39msandbox,\n\u001b[1;32m    509\u001b[0m             recorder\u001b[38;5;241m=\u001b[39mrecorder,\n\u001b[1;32m    510\u001b[0m             epochs_reducer\u001b[38;5;241m=\u001b[39mepochs_reducer,\n\u001b[1;32m    511\u001b[0m             solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[1;32m    512\u001b[0m             tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    513\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    514\u001b[0m             score\u001b[38;5;241m=\u001b[39mscore,\n\u001b[1;32m    515\u001b[0m             debug_errors\u001b[38;5;241m=\u001b[39mdebug_errors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    516\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    517\u001b[0m         )\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;66;03m# exit the loop if there was a cancellation\u001b[39;00m\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m([result\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]):\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/_eval/run.py:132\u001b[0m, in \u001b[0;36meval_run\u001b[0;34m(run_id, tasks, parallel, eval_config, eval_sandbox, recorder, epochs_reducer, solver, tags, metadata, debug_errors, score, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m task_eval_config\u001b[38;5;241m.\u001b[39msample_id \u001b[38;5;241m=\u001b[39m resolve_task_sample_ids(\n\u001b[1;32m    127\u001b[0m     resolved_task\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39mname, task_eval_config\u001b[38;5;241m.\u001b[39msample_id\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# resolve the task scorers\u001b[39;00m\n\u001b[1;32m    131\u001b[0m eval_scorer_specs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 132\u001b[0m     [\u001b[43mas_scorer_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m scorer \u001b[38;5;129;01min\u001b[39;00m task\u001b[38;5;241m.\u001b[39mscorer]\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task\u001b[38;5;241m.\u001b[39mscorer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    135\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# resolve task metrics\u001b[39;00m\n\u001b[1;32m    138\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m     to_metric_specs(task\u001b[38;5;241m.\u001b[39mmetrics) \u001b[38;5;28;01mif\u001b[39;00m task\u001b[38;5;241m.\u001b[39mmetrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/scorer/_scorer.py:201\u001b[0m, in \u001b[0;36mas_scorer_spec\u001b[0;34m(scorer)\u001b[0m\n\u001b[1;32m    199\u001b[0m name \u001b[38;5;241m=\u001b[39m registry_unqualified_name(scorer)\n\u001b[1;32m    200\u001b[0m metrics \u001b[38;5;241m=\u001b[39m scorer_metrics(scorer)\n\u001b[0;32m--> 201\u001b[0m resolved_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m args \u001b[38;5;241m=\u001b[39m registry_params(scorer)\n\u001b[1;32m    204\u001b[0m metadata \u001b[38;5;241m=\u001b[39m deepcopy(registry_info(scorer)\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/venvs/venv_paperQA2/lib/python3.12/site-packages/inspect_ai/scorer/_scorer.py:228\u001b[0m, in \u001b[0;36mresolve_metrics\u001b[0;34m(metrics)\u001b[0m\n\u001b[1;32m    221\u001b[0m             resolved_metrics\u001b[38;5;241m.\u001b[39mappend(as_metric_spec(metric_item))\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m             resolved_metrics\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    224\u001b[0m                 {\n\u001b[1;32m    225\u001b[0m                     metric_group: [\n\u001b[1;32m    226\u001b[0m                         as_metric_spec(metric) \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics_list\n\u001b[1;32m    227\u001b[0m                     ]\n\u001b[0;32m--> 228\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m metric_group, metrics_list \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmetric_item\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()\n\u001b[1;32m    229\u001b[0m                 }\n\u001b[1;32m    230\u001b[0m             )\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resolved_metrics\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "from scorers.paperqa_scorer import paperqa_scorer\n",
    "\n",
    "# Mini Example with 1 working sample\n",
    "example = {\n",
    "    \"question\": litqa2_test_data[\"question\"][0],\n",
    "    \"ideal\": litqa2_test_data[\"ideal\"][0],\n",
    "    \"distractors\": litqa2_test_data[\"distractors\"][0]\n",
    "}\n",
    "\n",
    "sample = record_to_sample_custom(example)\n",
    "mini_dataset = MemoryDataset([sample])\n",
    "# mini_dataset.shuffle_choices()\n",
    "\n",
    "@task\n",
    "def paperqa_eval_mini():\n",
    "    return Task(\n",
    "        dataset=mini_dataset,\n",
    "        solver = bridge(paperqa_agent(template=MULTIPLE_CHOICE_TEMPLATE_CUSTOM, settings=paperqa_settings)),\n",
    "        scorer=paperqa_scorer(no_answer=UNCERTAIN_ANSWER_CHOICE),\n",
    "        epochs=Epochs(1, \"mode\")\n",
    "    )\n",
    "    \n",
    "# asyncio.run(eval(paperqa_eval_mini()))\n",
    "eval(paperqa_eval_mini())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_paperQA2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
